[{"content":"长话短说，直接贴图！ [部分收集自b站] 以下是图片及其对应生成输入（因为图片命名更改了格式，含有'_\u0026lsquo;字样，请自行处理一下或者写一个 str.replace()方法替换成空格,且吧末尾的200去除）\n A_beautiful_painting_of_a_romantic_and_elegant_wedding_at_sunset_A_bride_white_wedding_dress_A_church_by_the_sea_rose_beach_light_effect_Dream_Caspar_David_Friedrich_artstation_200 \n Light_effect_Dream_Galaxy_StarsGreg_Rutkowski_Unreal_engine_Artstation_Towering_waves_200 \n Splendid_colorful_fireworks_in_the_sky_desert_Dream_Karlsimon_Greg_Rutkowski_Unreal_Engine_James_Gurney_Artstation_200 \n The_mountains_follow_the_plains_and_the_river_flows_into_the_great_wildernessTrending_on_artstation_200 \n A_beautiful_painting_of_a_building_full_of_sakura_bridge_clouds_sunset_fairy_tale_Distant_town_light_effect_dream_Josan_Gonzalez_artstation_200 \n a_beautiful_painting_of_mysterious_hard_sci-fi_city_at_sunset_clouds_neon_light_Cyberpunk_Magnificent_Greg_Rutkowski_artstation_200 \n A_Moonlit_Night_On_The_Spring_River_Greg_Rutkowski_James_Gurney_Artstation_200 \nps:你可能难以想象，他的英文出自《春江花月夜》  The_peach_blossom_lover_plants_peach_trees_in_days_fine_Greg_Rutkowski_James_Gurney_Artstation_200 \nps:你可能难以想象，他的英文出自唐寅的《桃花庵歌》“桃花仙人种桃树”\n A_Moonlit_Night_On_The_Spring_Riverromantic_In_the_morning_Greg_Rutkowski_James_Gurney_Artstation_160 \nps:你可能难以想象，他的英文出自《春江花月夜》  Change_is_the_WayGreat_seas_have_turned_into_fieldsTrending_on_artstation_200 \nps:你可能难以想象，他的英文出自【人间正道是沧桑】\n Cherry_blossoms_grand_palaces_above_clouds_rivers_sunshine_dream_Greg_Rutkowski_James_Gurney_Artstation_200 \n Cherry_blossoms_village_river_sunshine_dream_Greg_Rutkowski_James_Gurney_Artstation_200 \n In_the_morning_light_some_ancient_Chinese_buildings_on_the_mountains_sakura_mysterious_and_serene_landscape_clouds_river_Ivan_Aivazovsky_artstation_200 \n","date":"2022-05-12T22:25:38+08:00","image":"https://sanbuphy.github.io/p/disco-diffusion%E5%85%B3%E9%94%AE%E5%AD%97%E5%A4%A7%E8%B5%8F/cherry_hu829ff27670732ab1c894e3f69a29609b_502882_120x120_fill_box_smart1_3.png","permalink":"https://sanbuphy.github.io/p/disco-diffusion%E5%85%B3%E9%94%AE%E5%AD%97%E5%A4%A7%E8%B5%8F/","title":"Disco Diffusion关键字大赏"},{"content":"// TODO: //加入协程相关内容 // 资源安全性\n// 进程、线程间通信方式 // 如何捕获子线程异常\n简单理解进程和线程 什么是进程 一个任务就是一个进程，是操作系统资源分配的基本单位。\n例如打开浏览器，打开word，打开游戏、QQ等等，都是独立的任务，它们各自为一个或者多个进程。\n这里要注意的是，同一种任务打开多个，分别属于不同进程，例如chrome打开多个标签，实际上它创建了多个进程。 “运行”的程序才可以称为进程。未运行的程序仅仅是一些指令和数据的集合，并非进程。\n什么是线程 线程可以看作一个任务的各项子任务，是操作系统直接的执行单元。\n例如播放器，既要解码视频、也要解码音频，所以在进程下存在多线程。在一个进程下一定存在一个线程，可以称它为主线程。\n小结 操作系统创建进程时，会单独为每一个进程分配各自的资源，进程与进程之间相互隔离。进程在执行过程中拥有独立的内存单元，而多个线程共享内存。 可见，操作系统执行的粒度是线程，分配资源的粒度是进程，我们的多任务操作系统，在单核CPU上是在各个线程上不断切换而达到目的，而在多核CPU上则多个任务可以创建多个进程来完成，同时也可以创建多个线程来完成，线程是操作系统直接的执行单元。能同时执行多个线程任务。 值得注意的是，在python中，由于全局解释器锁的存在，线程并没有发挥并行计算的作用，而是提供了并发的能力。（只能在一个cpu核上运行）\n进程与线程的比较 进程的优缺点 多进程的优点是稳定性好，一个子进程崩溃了，不会影响主进程以及其余进程。 但是缺点是创建进程的代价非常大 ，因为操作系统要给每个进程分配固定的资源，并且，操作系统对进程的总数会有一定的限制，若进程过多，操作系统调度都会存在问题，会造成假死状态。。不过，进程与进程之间是完全隔离的，进程A崩溃了完全不会影响到进程B。\n线程的优缺点 多线程优点是效率较高一些，但是致命的缺点是任何一个线程崩溃都可能造成整个进程的崩溃，因为它们共享了进程的内存资源池，没有自己单独的内存地址空间，指针数据的错误可以导致任何同地址空间内其他线程的崩溃，包括进程。\n进一步理解进程和线程（可选） 进程 进程的组成 当一个程序被载入内存并成为一个进程后，它会占用一部分存储空间，此空间会分为 4 个区域：   这 4 个区域的作用分别是： 栈（Stack）：存储局部变量、函数参数等临时数据。 堆（Heap）：进程在执行期间可以动态申请这部分空间。 数据区（Data）：存储全局变量和静态变量。 文本区（Text）：存储进程要执行的机器指令代码。\n进程的生命周期 python中的GIL全局解释器锁 TODO:\n检查个人电脑的最大进程、线程数 不同电脑的配置状况决定了一个系统能够运行多少进程以及对应的线程数，简单粗暴的方法是用实例代码来检测到底能运行多少进程和线程：（一般情况下不要过于离谱即可） 不过需要注意的是，不同任务处理所需要占用的内存和cpu使用率是不同的，需要具体情况具体分析，但通常情况下的使用不会出现大问题（除非你考虑到数据共享安全性，想让一组线程执行完后再启动下一轮，那就要根据实际情况设计最大线程并加上线程锁；等到获取到的这些数据处理后才能继续处理下一轮数据；或者使用大小限定队列与线程池）\t多进程测试代码 首先，你可以用这个语句（linux与mac）进行直接查看:ubuntu\u0026gt; ps aux | wc -l,如果查询失败或者想要看到更直观的结果可以使用以下代码：\n#!/usr/bin/python import os import sys import re import threading import signal import time g_exit = 0 num = 0 def sig_process(sig, frame): global g_exit g_exit = 1 def sub_process(data): while not g_exit: time.sleep(1) print data def process(): num = int(sys.argv[1]) all_process = [] for i in range(num): try: pid = os.fork() except: pid = -1 if pid \u0026lt; 0: print 'error in fork' all_process.append(-1) elif 0 == pid: sub_process(i) os._exit(0) else: all_process.append(pid) while not g_exit: time.sleep(100) for i in range(num): if -1 == all_process[i]: continue os.waitpid(all_process[i], 0) def main(): if len(sys.argv) != 2: print 'wrong number parameter' return 0 signal.signal(signal.SIGINT, sig_process) process() if __name__ == '__main__': main() 多线程测试代码\n#!/usr/bin/python import os import sys import re import threading import signal import time g_exit = 0 num = 0 def sig_process(sig, frame): global g_exit g_exit = 1 def sub_process(data): while not g_exit: time.sleep(1) print data def process(): num = int(sys.argv[1]) all_thread = [] for i in range(num): try: td = threading.Thread(target = sub_process, args=(i,)) td.start() except: all_thread.append(-1) continue all_thread.append(td) while not g_exit: time.sleep(100) for i in range(num): if isinstance(all_thread[i], int): continue all_thread[i].join() def main(): if len(sys.argv) != 2: print 'wrong number parameter' return 0 signal.signal(signal.SIGINT, sig_process) process() if __name__ == '__main__': main() Reference  Python多线程与多进程学习\u0026mdash;-概念 操作系统，进程，线程 线程崩溃是否会造成进程崩溃 python编程（你的电脑能够执行多少线程和进程） python主线程捕获子线程异常 操作系统  ","date":"2022-04-24T19:23:03+08:00","permalink":"https://sanbuphy.github.io/p/python%E4%B8%AD%E5%A4%9A%E8%BF%9B%E7%A8%8B%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%A6%82%E5%BF%B5%E7%9A%84%E8%BF%9B%E9%98%B6%E6%8E%A2%E8%AE%A8/","title":"python中多进程、多线程概念的进阶探讨"},{"content":"待进一步完善\u0026hellip;龟速填坑中\n评价指标的基石 所有复杂问题都是简单问题的重复， 为了理解所有指标，首先需了解“四大天王”与“三大护法”\n四大天王——TP|FP TN|FN 我们认为positive代表了阳性（或者说正样本）， 而Negative代表了阴性（负样本)，那么在二分类中我们根据猜测的对错很容易排列出2x2种情况：     True False     Positive TP(猜对了阳) FP(猜错了阳)   Negative TN(猜对了阴) FN(猜错了阴)    其中左半边表示真值和我的猜测一致（猜对了），右半边表示真值和我的猜测不一致（猜错了）；\n上半边表示猜测阳性的全体 $P$，下半边表示猜测阴性的全体$N$。\nTP+FN表示真值阳性的全体，TN+FP表示真值阴性的全体。\n三大护法——Pre、ACC、Recall 根据四大金刚，我们容易得出常见模型中的三个评价指标， (为什么不能只看一个呢？因为容易被“浮云遮望眼”)\nPre(Precision 精确率又称查准率) 精确和准确在中文上看起来是一个意思，但实际上有微小的区别； 我们可以通过投标的例子来理解什么是精确，什么是准确。  精确率是度量信噪的一种方式，是对偏差程度的反应（标准差大就不精确）  从上图可知，精确率高的数据其标准差较小，也就是“我投中了，投中的差别不大”;在二分类中或者说判断是否阳性中，我们关注的重点是在我对阳性的判断中，是否判断对了(存不存在错判阳性的偏差）；如果精确率高，就说明我预测阳性偏差小，阳性就是真的阳性，较少出差错。 总结来说，精确率高指的是“我对正例判断出差错（偏差）的情况怎么样？”，也就是： $$ Precision = \\frac{TP}{P} = \\frac{TP}{TP+FP} $$ 精确率越高，说明我对正例/阳性判断的偏差越小，也就是TP占总判断阳性中的比例越大。 这时候可能会有聪明的同学问了，那为什么我们不考虑对负例判断的偏差呢，比如构建一个TN/TN+FN的指标？ 其实是有的，一般的，Pre还有更深入的名字叫“positive predictive value (PPV)”，显然也有对应的“Negative predictive value (NPV) ” 只是因为负样本通常较少，正样本较多；所以把PPV作为一个更常用的指标，归为精确率。\nACC(Accuracy 准确率) 为什么说精准但“精确不一定准确呢”？\n你可以这么想，精密仪器显然是为了误差小，那精密仪器一定能得到准确结果吗？————答案是不一定，比如你没有调零!(做过大物实验的朋友都知道~) 没有调零就好像y=kx+b中多了一个b，也就是投标图像中的“整体偏移真值” （下图的上半部分） 虽然很精确，但很不准确！！  左半侧更不准确，右半侧更准确  准确率更容易理解，准确率考虑的是更“全局”的结果是否正确， 也就是在我所有预测的结果中，我到底预测的“好不好”， 即： $$ Accuracy = \\frac{TP+TN}{所有预测} = \\frac{TP+TN}{TP+FP+TN+FN} $$\nRecall(召回率又称查全率) 顾名思义，查全的意思就是“我查的全不全”，我是否把所有的阳性覆盖到了（阳性全体为P），\n$$ Recall = \\frac{TP}{阳性全体}= \\frac{TP}{TP+FN} $$ 有时候查全率很高，但精确率很低。 比如我有3个阳性，97个阴性。我预测100个全为阳性（此时P=100且TP=3，FP=97），0个阴性；此时查全率为：3/3=100% ！\n但此时精确率为：3%。。。。。这显然是没有意义的。 有时候精确率很高，但查全率很低。 比如我还是有3个阳性，97个阴性。我刚好预测对某1个为阳性（此时P=1且TP=1，FP=0），99个阴性；此时精确率为：100% ！ 但此时查全率为：33.333%。。。。。这显然也是意义不大的。 查全率与精确率存在互逆关系\n我们能够隐约发现查全率和精确率的互逆关系，但究竟是为什么呢？ 仔细观察可以发现，Recall和Pre只在分母有差别，其中Recall的分母是TP+FN【不变量】，而Pre的分母是TP+FP【可变量】。\n为了提高Recall，我们需要增大TP，而分母不变； 但Pre中如果TP增加了FP也会跟着增加（我需要判断的东西变多了，精确率会下降，错误的概率会变大），分子的TP增加小于分母的TP+FP总体的增加。所以查全率和精确率是互逆的。\nF1、RPC、Roc、AUC 在充分了解了四大金刚与三大护法后，我们进一步来研究他们的衍生变体：\nReference  机器学习基础\u0026mdash;分类与检测的评价指标\u0026mdash;AP，mAP，PRC曲线 分类问题中的一些指标概念-Roc|AUC|Pre|Recall|ACC|AP|mAP|F1总结 分类器的ROC曲线及相关指标（ROC、AUC、ACC）详解 MCC — 机器学习中优于F1-score和accuracy的一个性能评价指标 Data Science in Medicine — Precision \u0026amp; Recall or Specificity \u0026amp; Sensitivity?  ","date":"2022-04-12T20:53:41+08:00","permalink":"https://sanbuphy.github.io/p/%E8%AF%A6%E8%A7%A3%E6%9C%BA%E5%99%A8/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E8%A7%81%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/","title":"详解机器/深度学习中常见评价指标"},{"content":"笔者因为要使用opencv c++版的需求，需要给家里的电脑配c/cpp的运行环境， 如果是Linux无脑Clion即可，windows上稍微有一些麻烦，附上全流程：\n首先请按照这个博主的安装教程走一遍流程c++ c语言安装流程\n如果顺利的安装好环境，接下来就是运行环节，首先随便新建一个C或者cpp文件，可参考； C语言的版本：\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;windows.h\u0026gt; int main() { printf(\u0026quot;Hello World\\n\u0026quot;); system(\u0026quot;pause\u0026quot;); return 0; } C++的版本：\n#include \u0026lt;iostream\u0026gt; using namespace std; int main() { cout \u0026lt;\u0026lt; \u0026quot;Hello Vscode\u0026quot; \u0026lt;\u0026lt; endl; return 0; } 接下来就是最重要的部分，我们有两种办法编译好文件运行， 第一种办法（直接编译）： 在终端中进入对应文件夹，输入gcc xxx.c 或者g++ xxxx.cpp 即可将文件编译成exe，然后直接终端中输入exe文件名字即可查看效果！ 如果你想运行后直接看跳出的结果，可以按照开头知乎链接的答主方法操作。\n第二种方法（间接编译）： 我们要做两件事：\n 首先在上面菜单栏找到终端，选择配置生成任务，选择对应的文件（gcc或者g++ 有可能只显示c++ 但点进去后会出现gcc) 左侧找到运行与调试在上面框框中点开，然后选择工作区对应的文件夹添加配置（会生成.vscode文件夹，内部就是配置文件！），或者直接运行中找到添加配置，然后同样找到对应gcc g++即可（gcc或者g++ 有可能只显示c++ 但点进去后会出现gcc)  如果第二种方法配置出现问题了（launch或者task），你可以把.vscode文件夹删除，重新配置即可。\n","date":"2022-03-20T16:28:01+08:00","permalink":"https://sanbuphy.github.io/p/c/c-%E5%9C%A8windows%E4%B8%8B%E7%9A%84%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E6%B5%81%E7%A8%8B%E5%92%8C%E8%BF%90%E8%A1%8C%E6%96%B9%E6%B3%95/","title":"C/C++在Windows下的环境配置流程和运行方法"},{"content":"找工作的怪象与形成的本源【最重要的】 产业分布——我该去哪？ 一线：互联网企业、高新技术企业 ———— 房价很高 容不下肉身 二三线：事业单位、国企、各种你懂编制（研究所、老师。。。。。） —————— 房价不算特别高 可能容不下灵魂  建议对自己想去的地方有个目的性，趁早考虑。\n学校和社会生活的差距 思想上的准备落后  1. 考公务员 2. 当老师 3. 事业单位 。。。。。 其实是对找工作没有概念 【学校和社会生活的差距导致围城】  社会上温情确实更少一些  1.抱大腿？大腿你遇不到，小腿不想理你 2.跨部门的事情，基本是踢皮球 3. *在外面很难找女朋友* 1. 考虑现实 2.考虑远近  中心思想：你要真正断奶，为自己负责，为自己选择（来自家里人的建议，来自老师的建议，来自同学的建议）。\n一个毛病————追求生活的最优解 ——————不可能！ 生活其实没有最优解，只有不同阶段的局部最优 ——————追随心的声音\n问题：我们应该用现在的回报去估计将来的收入曲线吗？ 预测基本不可能——————追随心的声音\n怎么更好的获得回报/获得资料(工作岗位的需求) 假设：现在我要去追求二线的某某工作： 1、我要转行吗？做什么？怎么做？ 我是电子信息硕士生，不转行\n2、我要做本行吗？做什么？怎么做？\n第一步：信息获取 【获得信息是第一步的关键！】\n 我导师研究的是什么，我的老师有什么资源——（学术的资源、人脉的资源（公司的联系）） 【我能获得什么？】   获得对应的学识 获得对应公司的内推机会 获得学长学姐的信息 了解公司的内部好坏和福利（比如宁德时代的一些黑历史）   学长学姐和历年就业去向表————找出最优秀的学长学姐 问他们是怎么过的，怎么努力的，怎么去获取资源的（可以请咖啡或者约面谈或者邮件或者微信，相信同校的学长学姐！）  第二步：选择方向（避免最坏的情况，追求好一些的情况，但不存在最好的情况） 电气工程师？做算法工程师？去国家电网？还是去当小学老师？\n 给出可能的几个方向 求交集 各种找工作的网站 按照 1、关键词 2、城市 遍历每一家公司的招工需求—————获得待遇情况，或者是去搜索待遇曲线以及和学长学姐信息对比。  第三步：根据方向 努力！获取信息后努力！ 【根据社招的要求，去做校招，千万不要沦落到社招！】 【现在的国情就是校招yyds，应届生yyds，请搞清楚应届生有什么福利（落户政策，人才引进)!!!!!!!!!!!!!!!!!!!!】 【必须利用好应届生的身份，否则会后悔一辈子】\n  自己做个文档（工作方需要什么经历，看重学生的什么素质，招工需要什么技术栈(ASIC\\会验证会测试\\会FPGA\\会python使用EDA验证等） 举个例子： 岗位职责： 1、深入了解ASIC设计及验证流程，可根据芯片设计规格书，编写验证需求，可搭建验证平台并完成验证用例和回归; 2、执行验证计划，编写验证用例，开展递归测试，完成问题的调试和修复； 3、负责芯片IP集成验证以及IP模块验证，有高速接口验证经验者优先; 4、负责SOC系统级验证，并收集、分析和提高验证覆盖率; 5、熟悉FPGA芯片架构，可协助测试工程师完成芯片测试工作。 任职要求： 1、微电子电子工程通信计算机等相关专业硕士及以上学历； 2、熟悉system *** 、OVMUVM等能独立搭建可重用的系统验证平台; 3、熟练掌握perl 、python等脚本语言熟练使用EDA验证工具(如VCS、NCsim等)； 工作认真、积极主动、严谨、敬业、有较强的沟通能力与团队组织协调能力。福利待遇：国企福利，六险二金，做五休二，餐费补助，交通补助，全勤奖，项目津贴，年终奖，健身房，羽乒馆等\n  针对技术栈去努力提高，顺便找志同道合的人【群友、交流群、学长学姐（拉学弟学妹下水）】\n  第四步：社交同样重要！  学长学姐 交流群 群友。。。。。  为什么要实习   面试造火箭——实际上拧螺丝 【台积电的例子】\n 如果在小厂。。。你可能要一个人干三个人的活 如果在大厂。。。拧螺丝！    公司需要的是啥（什么价值？需要你产生什么价值，给公司带来什么，自己的可替代性强不强）\n  弥补学校与社会的鸿沟\n  攒经验，刷履历\n  向小朱学习！！！\n  舔狗的艺术 （找一切机会向优秀的大佬们学习）\n 比起以后不能多请老婆喝一杯奶茶的委屈，比起以后不能多给儿子买一根棒棒糖的委屈，自己舔狗又有什么委屈的？ 说不定就舔到了呢？ 都是小概率事件，值得尝试！ 不被理睬才是最正常的现象，如果有就带上感谢！没有也没关系，这很正常！  面子的问题怎么解决————反正几十年后大家都入土了，没什么好怕的 战胜遗忘的方法：  复习 复习 复习 （推荐一款app memory helper 根据遗忘曲线安排你应该什么时候复习什么 免费！） 干中学为什么忘得慢？ 因为你天天都得干  做这些的目的是。。。？  青春一去不复返！ 获得更好的报酬 看到更好的自己 活在短暂未来的自己 比起炒股，你能做好的也就是这些了  ","date":"2022-02-21T21:18:24+08:00","permalink":"https://sanbuphy.github.io/p/%E8%80%83%E7%A0%94%E6%89%BE%E5%B7%A5%E4%BD%9C%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B/","title":"考研，找工作的那些事"},{"content":"中心思想：遇事不决，降低版本 （numpy,paddlepaddle,opencv）\n以防万一的操作 1.安装PyQt5\nsudo apt-get install python3-pyqt5\n2.安装qt-designer\n sudo apt-get install qt5-default qttools5-dev-tools\n#命令行输入designer可测试是否安装成功\n推荐安装流程（从新建虚拟环境到安装一条龙服务） 请严格按照顺序！\n#[装错了想卸载重装，把install改成uninstall 不要加包括-i在内后面的信息即可]\n#创建虚拟环境\nconda create -n 改成你的虚拟环境名字 python=3.7 【3.7别打3.9】\n#进入虚拟环境\nconda activate 改成刚才你的虚拟环境名字\n#装坏了想炸掉虚拟环境（慎重别打错了）\nconda remove -n 你的虚拟环境名字 --all\n#此时你能看到终端左侧显示环境名，接下来安装各类程序\n#注：这里采用百度源作为示范，你也可以使用清华源：\n -i https://pypi.tuna.tsinghua.edu.cn/simple\n#升级pip\npip install --upgrade pip -i https://mirror.baidu.com/pypi/simple\t\n#以防万一的操作\npip install qtpy -i https://mirror.baidu.com/pypi/simple \npip install pyqt5 -i https://mirror.baidu.com/pypi/simple \n#安装paddle github主页面 【这里采用的是cpu，如果要gpu详细安装过程参考gpu怎么看】\n（如果出现问题可以在paddlepaddle后面加上==2.1.0安装旧版本）\npython -m pip install paddlepaddle -i https://mirror.baidu.com/pypi/simple\n#安装PPOCRLabel（OCR） github主页面\n pip install PPOCRLabel -i https://mirror.baidu.com/pypi/simple\n pip install trash-cli -i https://mirror.baidu.com/pypi/simple\n#安装EISeg（图像处理） github主页面\n​\tpip install eiseg -i https://mirror.baidu.com/pypi/simple  #只输入这个就行\n​\tpip install paddleseg -i https://mirror.baidu.com/pypi/simple \n#【最后】安装旧版本opencv（如果装了请先卸载）\n pip install opencv-python==4.2.0.32 -i https://mirror.baidu.com/pypi/simple\n#接着测试ocr和eiseg：首先输入PPOCRLabel --lang ch\n#如果此时出现一段加载过程，最后出现界面，就说明你安装成功了！！\n#再次输入： eiseg 这时候大概率已经成功打开！\n倒霉人请看这 如果eiseg打开不成功可以尝试曲线方案（先安装这些依赖库）：\n(pip不成功就改成conda)\n#首先先把paddleseg的整个项目clone到本地 （以下安装还打不开可以这么做）\nconda install gdal\npip install qtpy\npip instal pyqt5\npip install easydict\npip install scikit-image\n这时候再进入即可！ （如果你还是打不开，就clone后进入eiseg子文件，按照这个帖子运行exe.py\t）\n最后使用conda deactivate退出虚拟环境！！\n","date":"2022-02-19T20:45:40+08:00","permalink":"https://sanbuphy.github.io/p/%E5%AE%8C%E7%BE%8E%E5%AE%89%E8%A3%85paddleppocrlabeleiseglinux/","title":"完美安装paddle、PPOCRLabel、eiseg(linux)"},{"content":"备注:我正在写一份目标检测\u0026amp;图像异常检测的综述slides，如果有兴趣可以等我出！欢迎邮件催更提建议：physicoada@gmail.com\nopencv基础： 推荐参考： 基于Python的Opencv全系列速成课 3天建立计算机视觉移动应用程序-支持iOS与Android 无人机编程与Python教学\n项目地址：https://github.com/jasmcaus/opencv-course\n几个大型综述  Object Detection in 20 Years: A Survey  相关笔记：https://zhuanlan.zhihu.com/p/192362333\n 综述：目标检测二十年（2001-2021） 目标检测近5年发展历程概述，从R-CNN到RFBNet（2013\u0026ndash;2018） (韩国人整的) 目标检测：Anchor-Free时代 - 陀飞轮的文章 - 知乎 https://zhuanlan.zhihu.com/p/62103812 CVPR 2021 论文大盘点-目标检测篇 - 我爱计算机视觉的文章 - 知乎 https://zhuanlan.zhihu.com/p/387510116 目标检测的精进路径 - mileistone的文章 - 知乎 https://zhuanlan.zhihu.com/p/266648028 目标检测入门，看这篇就够了（上） - 最刚烈的文章 - 知乎 https://zhuanlan.zhihu.com/p/60120331 国内做深度学习目标检测的有哪些大牛和厉害的课题组？ - Amusi的回答 - 知乎 https://www.zhihu.com/question/330390445/answer/723973941  ICCV 2021 结果出炉！最新200篇ICCV2021论文分方向汇总（更新中） https://zhuanlan.zhihu.com/p/392575669\nhttps://zhuanlan.zhihu.com/p/354043252\n7.CVPR2021论文分方向盘点 https://github.com/extreme-assistant/CVPR2021-Paper-Code-Interpretation#7\n 一文看尽 27 篇 CVPR2021 2D 目标检测论文 https://mp.weixin.qq.com/s/Ho7qtrpF9FhHGaamkQo6Lw  CVPR 2020 论文大盘点-目标检测篇\nhttps://bbs.cvmart.net/articles/2732\n其他  《目标检测》-第24章-YOLO系列的又一集大成者：YOLOX！https://zhuanlan.zhihu.com/p/391396921 目标检测可以先从成熟框架开始上手，比如mmdetection和detectron2。 如果基础。。。。。。目标检测该怎么学呀，目前研一，老师啥也不会，感觉毕不了业了？ - 小小将的回答 - 知乎 https://www.zhihu.com/question/510784176/answer/2305603811 系统地学习目标检测可以遵从下面的学习路线： 1.学习经典工作。经典工作包括RCNN系列（RCNN、Fast RCNN、Faster RCNN），宏观上可以学习到什么是目标检测、目标检测是做什么的，微观上可以学习到诸如Region Proposal Network（后续one-stage工作的基础）、Anchor box等基础技术。这个系列后来被划定为“two-stage”工作，检测精度好、速度要慢一些。随后，再学习早期的YOLO系列工作（YOLOv1、YOLOv2），宏观上可以学习到什么是one-stage目标检测方法、如何进行端到端的训练和推理，同时，学习SSD，可以初次接触到多级检测方法——使用更多的特征图去检测不同大小的物体。最后，学习FPN、YOLOv3以及RetinaNet（Focal loss），掌握当下主流检测框架“分而治之”方法。学习玩这些经典工作，最好能从中挑选出一至两个工作，进行复现，那么，目标检测就入门了。目标检测该怎么学呀，目前研一，老师啥也不会，感觉毕不了业了？ - Kissrabbit的回答 - 知乎 https://www.zhihu.com/question/510784176/answer/2305881442 目标检测（Object Detection）入门概要https://blog.csdn.net/f290131665/article/details/81012556  Reference  目标检测位置回归损失函数整理 目标检测回归损失函数简介：SmoothL1/IoU/GIoU/DIoU/CIoU Loss 边框回归(Bounding Box Regression)详解 Faster RCNN 中检测框位置回归是怎么做的 目标检测（1）-Selective Search 第三十三节，目标检测之选择性搜索-Selective Search Object Detection\u0026ndash;RCNN,SPPNet,Fast RCNN，FasterRCNN论文详解 什么是anchor-based 和anchor free？   ","date":"2022-02-08T15:28:17+08:00","permalink":"https://sanbuphy.github.io/p/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%9B%B8%E5%85%B3%E8%B5%84%E6%96%99%E6%B1%87%E6%80%BB/","title":"目标检测相关资料汇总"},{"content":"在介绍具体的算法之前，我们得先认识一下神经网络以及为什么要构造这样的神经网络。 注，如果你对神经网络的发展史感兴趣，可参考神经网络浅讲：从神经元到深度学习\n神经网络基础 首先，我们知道人的神经网络是由神经元组成的，每个神经元与其他神经元相连。当某个神经元 “兴奋”时, 就会向相连的神经元发送化学物质，改变这些神经元内的电位；如果某神经元的电位超过了一个“阈值” ( threshold), 那么它就会被激活，即 “兴奋” 起来，接着向向其他神经元发送化学物质。 我们可以把神经元抽象成“神经单元”，即从单纯的输入0或1，阶跃函数型激活模式，输出0或1（即点火）的生物模型；变为允许输入任何模型框架内数值，自选激活函数且将结果转化为兴奋度的一般模型。 一般的，我们常用sigmoid函数作为激活函数，如果你想了解其他激活函数，可以自行搜索（网络上真的很多）：  MP神经元模型 by：周志华-机器学习   常见激活函数 by：周志华-机器学习  由上可知，神经单元可以拆解为三大要素： $$ \\text{输入：} z=w_1x_1+\\cdots +w_nx_n+b\\cdot 1 \\\\ \\text{输出：} a=f\\left( z \\right) \\\\ \\text{激活函数：}f\\left( z \\right) =\\frac{1}{1+e^{-z}} $$ 其中b为偏置（为了形式美观，我们令阈值 $ \\theta = -b $ ），b·1的意思是常数1作为b对应的输入值（这样可以在之后将等式写成更美观的矩阵，类似多元回归时把常数放入矩阵的操作） 有了神经单元，把他们连接后就形成了神经网络（不同层的作用将在下部分给出）  阶层型神经网络 by：深度学习的数学 \n神经网络的运作原理 假设，你是一只爱吃“0”与“1”图样猫粮且非常挑食的小猫，出于懒惰，你想让计算机帮自己把这两类图案的猫粮完全区分。在以前，我们学过了分类法的线性回归(逻辑回归)，单纯的二维数据点分类不是件难事，但当我们遇到图像后该怎么办呢？为了方便，我们需要引入上述的神经网络。 为了引入神经网络，首先需要对猫粮图案处理成计算机可识别的内容，我们可以这样进行编号：（举例图案为4x3的格子）  将猫粮的0/1图案使用格子划分  其中编号代表有多少个信号源输入，信号输入的大小可以根据黑白设定，给出相应的0/1。（比如黑格子给神经元1的输入，白格子为0的输入）  将图像信息输入神经网络  在这里为了简化，我们只有简单的三层，一层输入，一层隐藏层（你可以理解为进一步处理数据），一层输出（用于判断结果是0还是1）。我们把它称为两层神经网络（有些地方觉得输入层需要计入，但在此我们采用“因为输入层不活动只是输入”的观点，将输入层排除在层数计算外） 有了数据输入，接下来便需要进一步认识神经单元的符号语言，从而理解数据在神经网络中是怎么传导的。（此处友情建议复习小节一的神经单元三大要素）  神经单元的符号语言  如何理解这套符号呢？如图所示，“我们现在所研究的”对象是第L层的第j个神经单元，而他前一层的神经网络即为L-1层，且我们设前一层神经单元的标号为i。根据这套符号以及我们前面学到的神经单元三大要素，我们知道了第L层第j个神经单元的输入为 $ \\mathrm{z}_{j}^{l} $，输出为 $ \\mathrm{a}_{j}^{l} $ ,其中b为偏置，且L-1层第i个神经单元对L层第j个神经单元的影响权重为 $\\mathrm{w}_{ji}^{l}$，激活函数负责把神经单元的“左半边”变成右半边的输出。（注，有些地方权重编号的ji可能互换为ij，但不影响最后结果）\n有了神经网络，我们自然想到它到底有什么用？让我们回望初心——分类，我们的目的是通过模型对样本进行分类；操作是通过对训练集的学习，将某种参数决定下的模型预测值（某种特征）与对应的现实特征比较，让现实和预测的差别尽量小，从而实现分类的效果。而神经网络也是如此，决定分类结果的是输出层，我们可以设输出层（参考上面出现的两层神经网络）的第一个神经单元的输出值 $a_1^3$ 在图样为1的时候接近1（现实目标为1），第二个神经单元的输出值 $a_2^3$在图样为0的时候接近1（显示目标为0），即为：（注意，值为1表示的是神经单元“兴奋”）\n为什么需要梯度下降法 前向传播原理 反向传播原理 一般化处理 浅析西瓜书中的BP算法 Reference  温故知新——前向传播算法和反向传播算法（BP算法）及其推导 反向传播算法详细推导 深度学习之反向传播算法 上/下 Part 3 ver 0.9 beta 《深度学习的数学》 涌井良幸 涌井贞美 《机器学习 Machine Learning》 周志华   ","date":"2022-01-29T16:18:00+08:00","image":"https://sanbuphy.github.io/p/%E5%B0%8F%E7%8C%AB%E4%B9%9F%E8%83%BD%E7%9C%8B%E6%87%82%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/jeff-rodgers-DaPabnoYMKc-unsplash_hu994d601531656c23f7e611d455c8d878_8153127_120x120_fill_q75_box_smart1.jpg","permalink":"https://sanbuphy.github.io/p/%E5%B0%8F%E7%8C%AB%E4%B9%9F%E8%83%BD%E7%9C%8B%E6%87%82%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/","title":"小猫也能看懂的反向传播算法"},{"content":"首先，我们需要认识什么是信息量与信息熵:\n自信息量 通常自信息可以从两个方面来理解:\n 自信息是事件发生前,事件发生的不确定性。 自信息表示事件发生后,事件所包含的信息量。  (比如你看到这，会想问作者也许不是一只猫而是人类————那我当然不是猫，这就没有不确定性可言，没有什么信息量；如果有一天我真的是猫了，那便是大新闻了！！) 自然而然，我们会想到所谓信息量应当与概率有关，且应该可以加合（两个事件发生时带来的信息量应该是分别发生时的和），也就是满足以下特质：\n $f\\left( P \\right) \u0026gt;0 \\; \\; \\; \\; P\\left( x \\right) \\in \\left( 0,1 \\right) $ $f\\left( P_A·P_B \\right) =f\\left( P_A \\right) +f\\left( P_B \\right) $ $f\\left( 1 \\right) =0$ 事件发生概率越大，自信息量越小  此时我们可以才想到对数会满足这样的性质，于是可以给出：$I\\left( x \\right) =-\\log \\left( P\\left( x \\right) \\right) $ 因为在计算机领域中习惯用二进制，所以我们通常以2为底，这样自信息量的单位就为比特bit,——即二进制数的一位包含的信息或2个选项中特别指定1个的需要信息量。而机器学习中常选择以e为底，单位为奈特nats 你可以通过以下例题来更好的理解自信息量： 以2为底的对数符号lb   \n信息熵 接下来，我们将进一步研究什么是信息熵，在前面我们学会了如何衡量一个事件的不确定性，但一个随机变量可能包含的多个事件，我们该如何对这个 随机变量的不确定性 进行刻画呢？ 我们会自然想到求出所有事件的信息量期望，且熵越大，事件的不确定性越强，当满足均匀分布时熵最大(有约束情况下要额外考虑，一阶矩二阶矩不同时的最大熵分布不同，详情可参考最大熵原理）；如果熵值小，证明某个事件发生的概率比较大，随机变量取某个值的概率大，不确定性就小了。(另外，信息熵也可以理解为解除信源不确定性所需要的信息量) 于是我们给出： (其中规定)$0\\log 0=0$ $$ H\\left( x \\right) =-\\sum_{i=1}^n{p\\left( x_i \\right) \\log \\left( p\\left( x_i \\right) \\right)} $$ 我们可以验证，当n个事件满足等概率分布时其中当结果为logn（n为总数）信息熵达到最大值。 另外可以给出条件熵(你可以运用条件概率辅助理解)： $$ H\\left( Y|X \\right) =-\\sum_x{\\sum_y{p\\left( xy \\right) \\log P\\left( y|x \\right) =\\sum_x{-\\sum_y{P\\left( y|x \\right) \\log P\\left( x \\right) =\\sum_x{P\\left( x \\right) H\\left( Y|x \\right)}}}}} $$\n相对熵 如果我们对于同一个随机变量x有两个单独的概率分布P(x) 和 Q(x)，我们可以使用KL散度（Kullback-Leibler (KL) divergence）或者叫相对熵来衡量这两个分布的差异情况，其中p对q的相对熵写作(在机器学习中，我们可以把P(x)看作真实分布，而Q(x)作为预测的分布)： $$ D_{KL}\\left(p||q \\right) =\\sum_x{p\\left( x \\right) \\log \\frac{p\\left( x \\right)}{q\\left( x \\right)}=E_{p\\left( x \\right)}\\log \\frac{p\\left( x \\right)}{q\\left( x \\right)}} $$ 同时KL散度还满足以下条件： $$ D_{KL}\\left(p||q \\right) \\ne D_{KL}(q||p) \\\\ D_{KL}\\left( p||q\\right) \\geqslant 0 $$ 对于第一个式子，我们可以借助以下内容理解：  by Deep Learning.Ian Goodfellow and Yoshua Bengio and Aaron Courville  用比较通俗的话来说，让我们回到公式之中，且注意到P(x)作为真实分布，Q(x)作为预测的分布；\n$$ D_{KL}\\left( p||q \\right) =E_{p\\left( x \\right)}\\log \\frac{p\\left( x \\right)}{q\\left( x \\right)} $$\n 当第一种情况，如果P(x)是较大的，那么q(x)也应该较大来保证相对熵最小化；如果P(x)是较小的，那实际上q(x)的大小对相对熵影响不大；所以我们只需要特别注意前者的情况。此时在看图你就可以更加理解了。  $$ D_{KL}\\left( q||p \\right) =E_{q\\left( x \\right)}\\log \\frac{q\\left( x \\right)}{p\\left( x \\right)} $$\n当第二种情况，很显然会与第一种情况相反，如果P(x)是较小的，那么q(x)也应该较小来保证相对熵最小化————这就是为什么说图中提到概率小的地方比较重要，而q(x)较大的时候就影响不大了。  交叉熵 接下来我们要了解常用的一种更常用的熵————交叉熵，由前面学到的相对熵可以进一步推导：\n$$ \\begin{aligned} D_{KL}\\left( p||q \\right) \u0026amp;=\\sum_{i=1}^n{p\\left( x_{\\mathrm{i}} \\right) \\log \\left( \\frac{p\\left( x_i \\right)}{q\\left( x_i \\right)} \\right)}\\\\ \\mathrm{ }\u0026amp;=\\sum_{i=1}^n{p\\left( x_i \\right) \\log \\left( p\\left( x_i \\right) \\right)}-\\sum_{i=1}^n{p\\left( x_i \\right)}\\log \\left( q\\left( x_i \\right) \\right)\\\\ \u0026amp;=-H\\left( p \\right) -\\sum_{i=1}^n{p\\left( x_i \\right)}\\log \\left( q\\left( x_i \\right) \\right)\\ \\end{aligned} $$\n其中第一项我们可通过推导得知是针对真实分布概率p(x)的信息熵，而后一项我们定义为交叉熵； $$ H\\left( p,q \\right) =-\\sum_{i=1}^n{p\\left( x_i \\right)}\\log \\left( q\\left( x_i \\right) \\right) $$ 交叉熵可以理解为，消除体系不确定性所需要付出的努力大小。\n交叉熵与极大似然估计的联系 由于真实分布的信息熵是确定的，在优化过程中（最小化相对熵），我们可以把他忽略，只看交叉熵的部分。此外，最小化交叉熵其实与极大似然估计是等价的，具体证明如下：（参考Deep Learning.Ian Goodfellow and Yoshua Bengio and Aaron Courville） 我们考虑一组含有m个样本的数据集$\\mathbf{X}=({ x^{(1)},\\cdots ,x^{(m)} }) $,此时可以定义 $ \\theta $ 的极大似然为(其中P为模型的联合概率)：(如果你不懂argmax是什么意思可以参考argmax科普) $$ \\begin{aligned} \\mathbf{\\theta }_{ML}\u0026amp;=\\underset{\\theta}{\\mathrm{argmax}}P_{model}\\left( \\mathbf{X};\\theta \\right)\\\\ \\mathrm{ }\u0026amp;=\\underset{\\theta}{\\mathrm{argmax}}\\prod_{i=1}^m{P_{model}\\left( \\boldsymbol{x}^{\\left( i \\right)};\\theta \\right)} \\end{aligned} $$ 由于乘积不好计算，我们可以取log将他转换为加和形式，取最值时的参数不变；且可以乘上不影响结果的 $ \\frac{1}{m} $。 $$ \\begin{aligned} \\mathbf{\\theta }_{ML}\u0026amp;=\\underset{\\theta}{\\mathrm{argmax}}\\sum_{i=1}^m \\mathrm{log} {P_{model}\\left( \\boldsymbol{x}^{\\left( i \\right)};\\theta \\right)}\\\\ \\mathrm{ }\u0026amp;=\\underset{\\theta}{\\mathrm{argmax}}\\frac{1}{m}\\sum_{i=1}^m \\mathrm{log} {P_{model}\\left( \\boldsymbol{x}^{\\left( i \\right)};\\theta \\right)} \\end{aligned} $$ 由大数定律可知（算术平均值依概率收敛于期望）： $$ \\frac{1}{m}\\sum_{i=1}^m{X_i\\longrightarrow} \\mu $$ 可以将原式进一步化为： $$ \\begin{aligned} \\mathbf{\\theta }_{ML}\u0026amp;=\\underset{\\theta}{\\mathrm{arg}\\max}\\mathbb{E}_{\\mathbf{x}~\\hat{p}_{data}}\\log P_{model}\\left( \\boldsymbol{x};\\boldsymbol{\\theta } \\right)\\\\ \u0026amp;=\\underset{\\theta}{\\mathrm{arg}\\max}\\sum_x{p\\left( x \\right) \\log q\\left( x \\right)}\\\\ \u0026amp;=\\underset{\\theta}{\\mathrm{arg}\\min}\\left[ -\\sum_x{p\\left( x \\right) \\log q\\left( x \\right)} \\right]\\\\ \\end{aligned} $$ Bravo!!! 此时你惊喜的发现这就是我们前面推导得到的交叉熵公式，至此，对于真实分布和模型分布，我们明白了MLE方法（让似然最大化）等价于两者间交叉熵的最小化。好奇的你也许想问“MLE与KL散度也是共通的吗？”————这个问题你可以自己试试看，就用上式类似办法加常数即可！\nReference  一文搞懂交叉熵损失 详解机器学习中的熵、条件熵、相对熵和交叉熵  信息论基础 数字世界逼近现实世界——浅谈分布近似与最大似然估计  ","date":"2022-01-22T22:34:11+08:00","image":"https://sanbuphy.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80/michael-sum-LEpfefQf4rU-unsplash_hu05be89c43654817bfc3a6d1d1f1925fe_2269278_120x120_fill_q75_box_smart1.jpg","permalink":"https://sanbuphy.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80/","title":"机器学习中的信息论基础"},{"content":"本文为个人对西瓜书不成熟的一些理解和资料整理，欢迎批评指出意见，谢谢！(可邮箱联系physicoada@gmail.com)\n西瓜书章节三 线性模型 文字版浓缩可参考：周志华机器学习笔记 by:Vay-keen\n简略而言，本章主要涉及到三大块内容\n 线性回归及极大似然估计 对数几率回归及交叉熵思想 二分类线性判别分析  数学基础 极大似然估计 凸函数   本图来自：凸函数的四种判断方法 多元函数判别法(Hessian矩阵正定性)：\n机器学习中凸函数的好处：函数具有唯一的极小值。这意味着我们求得的模型是全局最优，不会陷入局部最优（想象一下无数波浪的函数，难以在梯度下降中找到最小值）。\n信息熵、相对熵与交叉熵 请参考数学专栏中的文章（机器学习中的信息论基础）\n拉格朗日乘子法 线性回归 矩阵推导 矩阵求导方法及其完整过程请参考我的另外一篇文章：矩阵求导建议入门手册\n极大似然估计法 对数几率回归(逻辑回归) 极大似然估计法 线性判别分析 多分类学习 ECOC码 在书中，我们会看到这样的一张图：   其中涉及到所谓的海明距离与欧氏距离。 海明距离指的是：对于长度相等的两个字符串，在相同位置上不同字符的个数。\n而欧氏距离指的是多维空间中两点间绝对距离： $ dist\\left( X,Y \\right) =\\sqrt{\\sum_{i=1}^n{\\left( x_i-y_i \\right) ^2}} $ 由此我们可以计算出图中的数字，对(a)图中可有： $$ \\qquad \\text{示例}-1 {\\color{red} -1 +1 -1 }+1 \\\\ C1\\text{编码} \\ -1 {\\color{red} +1 -1 +1 }+1 $$ 容易看出海明距离为3，而欧氏距离为 $$ \\sqrt{\\left( -1-\\left( -1 \\right) \\right) ^2+\\left( -1-1 \\right) ^2+2^2+2^2+0}=2\\sqrt{3} $$ 其他值可以用相同的办法推导得到。\nReference  逻辑回归的原理、推导和常见问题 凸函数、损失函数、线性模型的基本形式、线性回归、w* 的代码实现   ","date":"2022-01-17T22:53:41+08:00","permalink":"https://sanbuphy.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%BA%8C/","title":"《机器学习》西瓜书笔记(二)"},{"content":"初始介绍 符号规定 在本文中，我们做如下规定：\nMatrix矩阵为：$\\mathbf{A}, \\mathbf{X}, \\mathbf{Y}$ Vector向量（规定为$\\color{red} {列} $向量）为：$ \\mathbf{a}, \\mathbf{x}, \\mathbf{y}$\nScalar标量为：$a, x, y$\n分子布局 在矩阵求导中，我们有两种布局（分子与分母） 为了方便起见，本文只阐述了分子布局即：\n$$\\frac{\\partial \\mathbf{y}}{\\partial {x}}=\\left[\\begin{array}{c} \\frac{\\partial y_{1}}{\\partial x} \\\\ \\frac{\\partial y_{2}}{\\partial x} \\\\ \\vdots \\\\ \\frac{\\partial y_{m}}{\\partial x}\\end{array}\\right]\\ \\ \\ \\frac{\\partial y}{\\partial \\mathbf {x}} = \\left[\\frac{\\partial y}{\\partial x_{1}} ,\\frac{\\partial y}{\\partial x_{2}}, \\cdots ,\\frac{\\partial y}{\\partial x_{n}}\\right]$$ 分母布局为分子布局的转置。 记忆方法：分子列向量分母标量，看作长筒冰淇淋，分母看作小盒子，“能站住”。分子标量分母列向量，则盒子把冰淇淋“压倒了”。或可看最后结果的行数，是分子的行数便是分子布局。 一般的，我们会遇到如下布局,且可用记忆方法配合右图形象理解(下面是结果）：  by: Reference 2   by: Reference 2  当分子为矢量、矩阵时，结果为分子的行；当分子为标量时，结果是分母转置的行。\nVector-by-Vector 另外我们有：\n$\\mathbf{y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m \\end{bmatrix}$ $\\mathbf{x}=\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$ 由 $ \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}$ 运算后产生m行n列矩阵： $ \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\stackrel{\\text { def }}{=}\\left[\\begin{array}{cccc}\\frac{\\partial y_{1}}{\\partial x_{1}} \u0026amp; \\frac{\\partial y_{1}}{\\partial x_{2}} \u0026amp; \\ldots \u0026amp; \\frac{\\partial y_{1}}{\\partial x_{n}} \\\\ \\frac{\\partial y_{2}}{\\partial x_{1}} \u0026amp; \\frac{\\partial y_{2}}{\\partial x_{2}} \u0026amp; \\ldots \u0026amp; \\frac{\\partial y_{2}}{\\partial x_{n}} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ \\frac{\\partial y_{m}}{\\partial x_{1}} \u0026amp; \\frac{\\partial y_{m}}{\\partial x_{2}} \u0026amp; \\ldots \u0026amp; \\frac{\\partial y_{m}}{\\partial x_{n}}\\end{array}\\right] $ 这种矩阵可被称为Jacobian matrix。 接下来举个例子，若我们有： $$\\mathbf{y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\end{bmatrix} \\ \\ \\ \\mathbf{x}=\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\ \\ 且y_1=x^2_1-2x_2 \\ ,\\ y_2=x^2_3-4x_2$$ 则能得到: $$\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} =\\begin{bmatrix} 2x_1 \u0026amp; -2 \u0026amp; 0 \\\\ 0 \u0026amp; -4 \u0026amp; 2x_3 \\\\ \\end{bmatrix} $$\nMatrix-by-Scalar 同样的，我们可以给出矩阵与向量间的运算关系： $ \\frac{\\partial \\mathbf{Y}}{\\partial {x}} \\stackrel{}{=}\\left[\\begin{array}{cccc}\\frac{\\partial Y_{11}}{\\partial x} \u0026amp; \\frac{\\partial Y_{12}}{\\partial x} \u0026amp; \\ldots \u0026amp; \\frac{\\partial Y_{1n}}{\\partial x} \\\\ \\frac{\\partial Y_{21}}{\\partial x} \u0026amp; \\frac{\\partial Y_{22}}{\\partial x} \u0026amp; \\ldots \u0026amp; \\frac{\\partial Y_{2n}}{\\partial x} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ \\frac{\\partial Y_{m1}}{\\partial x} \u0026amp; \\frac{\\partial Y_{m2}}{\\partial x} \u0026amp; \\ldots \u0026amp; \\frac{\\partial Y_{mn}}{\\partial x}\\end{array}\\right]$ $ \\ \\ \\ \\ \\ and \\ \\ \\ \\ \\frac{\\partial y}{\\partial\\mathbf{X}} \\stackrel{}{=}\\left[\\begin{array}{cccc}\\frac{\\partial y}{\\partial X_{11}} \u0026amp; \\frac{\\partial y}{\\partial X_{21}} \u0026amp; \\ldots \u0026amp; \\frac{\\partial y}{\\partial X_{m1}} \\\\ \\frac{\\partial y}{\\partial X_{12}} \u0026amp; \\frac{\\partial y}{\\partial X_{22}} \u0026amp; \\ldots \u0026amp; \\frac{\\partial y}{\\partial X_{m2}} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ \\frac{\\partial y}{\\partial X_{1n}} \u0026amp; \\frac{\\partial y}{\\partial X_{2n}} \u0026amp; \\ldots \u0026amp; \\frac{\\partial y}{\\partial X_{mn}}\\end{array}\\right] $\n可以注意到当矩阵在分母时$\\mathrm{X}$已经“被转置”\n常用求导公式 注：其中$\\mathbf{a},\\mathrm{A}$都不是$\\mathbf{x}, \\mathrm{X}$的函数 $$\\frac{\\mathrm{d} \\mathbf{a}}{\\mathrm{d} x} =\\mathbf{0} \\tag{1} \\qquad (column \\ matrix)$$ $$\\frac{\\mathrm{d}a }{\\mathrm{d} \\mathbf{x}} =\\mathbf{0}^{\\mathrm{T}} \\tag{2} \\qquad (row \\ matrix)$$ $$\\frac{\\mathrm{d}a }{\\mathrm{d} \\mathbf{X}} =\\mathbf{0}^{\\mathrm{T}} \\tag{3} \\qquad (matrix)$$ $$\\frac{\\mathrm{d} \\mathbf{a} }{\\mathrm{d} \\mathbf{X}} =\\mathbf{0} \\tag{4} \\qquad (matrix)$$ $$\\frac{\\mathrm{d} \\mathbf{x} }{\\mathrm{d} \\mathbf{x}} =\\mathbf{I} \\tag{5} \\qquad (matrix)$$ 若想从“直观上”理解结果为什么会有转置符，可以反复理解 (1.2)分子布局 中的右图 $$\\frac{\\mathrm{d} \\mathbf{a}^{\\mathrm{T}}\\mathbf{x} }{\\mathrm{d} \\mathbf{x}}=\\frac{\\mathrm{d} \\mathbf{x}^{\\mathrm{T}}\\mathbf{a} }{\\mathrm{d} \\mathbf{x}}=\\mathbf{a}^{\\mathrm{T}} \\tag{6} $$ $$\\frac{\\mathrm{d} \\mathbf{x}^{\\mathrm{T}}\\mathbf{x} }{\\mathrm{d} \\mathbf{x}}=2\\mathbf{x}^{\\mathrm{T}} \\tag{7} $$ $$\\frac{\\mathrm{d} ({\\mathbf{x}^{\\mathrm{T}}\\mathbf{a}})^2 }{\\mathrm{d} \\mathbf{x}}=2\\mathbf{x}^{\\mathrm{T}}\\mathbf{a}\\mathbf{a}^{\\mathrm{T}} \\tag{8} $$ $$\\frac{\\mathrm{d} \\mathbf{Ax} }{\\mathrm{d} \\mathbf{x}} =\\mathbf{A} \\tag{9} $$ $$\\frac{\\mathrm{d} \\mathbf{x}^{\\mathrm{T}}\\mathbf{A} }{\\mathrm{d} \\mathbf{x}}=\\mathbf{A}^{\\mathrm{T}} \\tag{10} $$ $$\\frac{\\mathrm{d} \\mathbf{x}^{\\mathrm{T}}\\mathbf{A}\\mathbf{x} }{\\mathrm{d} \\mathbf{x}}=\\mathbf{x}^{\\mathrm{T}}(\\mathbf{A}+\\mathbf{A}^{\\mathrm{T}}) \\tag{11} $$\n注：其中(11)用到了矩阵求导中的\u0026quot;莱布尼兹法则\u0026quot;（仔细思考前者的行列与后者的行列就可以明白）: $$\\frac{\\partial \\mathbf u^{\\mathrm{T} }\\mathbf v}{\\partial \\mathbf x} = \\mathbf u^{\\mathrm{T}} \\frac{\\partial \\mathbf v}{\\partial \\mathbf x} + \\mathbf v^{\\mathrm{T}}\\frac{\\partial \\mathbf u}{\\partial \\mathbf x}$$\n实例练习 我们会好奇一个问题：为什么大多数求导后以及求导时形式都是转置在前？ 可以这么理解，假设有参数$\\mathbf{\\Theta } =\\begin{bmatrix} \\theta_0\\\\ \\theta_1\\\\ \\theta_2 \\end{bmatrix}$ 以及列向量$\\mathbf{x}=\\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\end{bmatrix}$ 我们可以把多元函数表达为$$f(x_1,x_2)=\\Theta^{\\mathrm{T}} \\mathbf{x}=\\theta_0 + \\theta_1x_1 + \\theta_2x_2$$ 方便计算，且符合目的。在计算结果直觉中一定要紧紧把握结果与分子（上下拉长）分母（左右拉宽）的关系。\n多元线性回归推导 在西瓜书第三章开头中，我们需要求解最小error$\\ E=(\\boldsymbol{y}-\\mathbf{X}\\boldsymbol{\\hat{w}})^{\\mathrm{T}}(\\boldsymbol{y}-\\mathbf{X}\\boldsymbol{\\hat{w}}) $ 对其展开有：$$ \\begin{aligned}(\\boldsymbol{y}-\\mathbf{X}\\boldsymbol{\\hat{w}})^{\\mathrm{T}}(\\boldsymbol{y}-\\mathbf{X}\\boldsymbol{\\hat{w}}) \u0026amp;=(\\boldsymbol{y}^{\\mathrm{T}}-\\boldsymbol{\\hat{w}}^{\\mathrm{T}}\\mathbf{X}^{\\mathrm{T}})(\\boldsymbol{y}-\\mathbf{X}\\boldsymbol{\\hat{w}}) \\\\ \u0026amp;= \\boldsymbol{y}^{\\mathrm{T}} \\boldsymbol{y}- (\\boldsymbol{y}^{\\mathrm{T}} \\mathbf{X}) \\boldsymbol{\\hat{w}}-\\boldsymbol{\\hat{w}}^{\\mathrm{T}} (\\mathbf{X}^{\\mathrm{T}} \\boldsymbol{y}) +\\boldsymbol{\\hat{w}}^{\\mathrm{T}} (\\mathbf{X}^{\\mathrm{T}} \\mathbf{X}) \\boldsymbol{\\hat{w}} \\end{aligned}$$\n把括号中看作常数，每一项分别对 $\\boldsymbol{\\hat{w}}$ 求导，利用公式$(9),(10),(11)$可得: $$ \\frac{\\partial \\boldsymbol{E}_{\\boldsymbol{\\hat{w}}} }{\\partial \\boldsymbol{\\hat{w}}} = -\\boldsymbol{y}^{\\mathrm{T}} \\mathbf{X} - \\boldsymbol{y}^{\\mathrm{T}} \\mathbf{X} + \\boldsymbol{\\hat{w}}^{\\mathrm{T}} [\\mathbf{X}^{\\mathrm{T}} \\mathbf{X}+(\\mathbf{X}^{\\mathrm{T}} \\mathbf{X})^{\\mathrm{T}}] = -2\\boldsymbol{y}^{\\mathrm{T}} \\mathbf{X} + 2\\boldsymbol{\\hat{w}}^{\\mathrm{T}} \\mathbf{X}^{\\mathrm{T}} \\mathbf{X} $$ 令其等于零可得： $$\\boldsymbol{y}^{\\mathrm{T}} \\mathbf{X} = \\boldsymbol{\\hat{w}}^{\\mathrm{T}} \\mathbf{X}^{\\mathrm{T}} \\mathbf{X}$$ 同时转置可得： $$ \\mathbf{X}^{\\mathrm{T}} \\boldsymbol{y} = \\mathbf{X}^{\\mathrm{T}} \\mathbf{X} \\boldsymbol{\\hat{w}} $$ 若此时 $\\mathbf{X}^{\\mathrm{T}} \\mathbf{X}$ 为满秩、非奇异矩阵，我们可以得到： $$\\boldsymbol{\\hat{w}}=( \\mathbf{X}^{\\mathrm{T}} \\mathbf{X})^{-1} \\mathbf{X}^{\\mathrm{T}} \\boldsymbol{y} \\\\ \\ \\\\ Q.E.D$$\n上文公式的证明过程  如果你不知道计算结果是否正确，可以使用验算矩阵求导结果是否正确进行验证。    证明(6) $$ Let \\; s=\\boldsymbol{a}^T\\mathbf{x}={a}_1x_1+\\cdots +a_nx_n.\\quad Then,\\;\\frac{\\partial s}{\\partial x_i}=a_i \\\\ So,\\:\\frac{\\mathrm{d} \\boldsymbol{a}^{\\mathrm{T}}\\mathbf{x} }{\\mathrm{d} \\mathbf{x}}=\\frac{\\mathrm{d} s}{\\mathrm{d} \\mathbf{x}}=\\left[ \\frac{\\mathrm{d}s}{\\mathrm{d}x_1},\\frac{\\mathrm{d}s}{\\mathrm{d}x_2},\\cdots ,\\frac{\\mathrm{d}s}{\\mathrm{d}x_n} \\right] =\\left[ a_1,a_2,\\cdots ,a_n \\right] =\\boldsymbol{a}^T $$\n  证明(7) $$ Let \\; s=\\boldsymbol{\\mathbf{x}}^{\\mathrm{T}}\\mathbf{x}=\\sum_i{x_{i}^{2}}. \\quad Then,\\;\\frac{\\partial s}{\\partial x_i}=2x_i \\\\ So, \\;\\; \\frac{\\mathrm{d} s}{\\mathrm{d} \\mathbf{x}}=2\\mathbf{x}^{\\mathrm{T}} $$\n  证明(8) 当成复合函数即可，相信你可以自己证明！\n  证明(9) 比较麻烦的方法，由： $$ \\mathbf{A}\\mathbf{x}=\\left[ \\begin{matrix} a_{11}\u0026amp;\t\\cdots\u0026amp;\ta_{1n}\\\\ \\vdots\u0026amp;\t\\ddots\u0026amp;\t\\vdots\\\\ a_{n1}\u0026amp;\t\\cdots\u0026amp;\ta_{nn}\\\\ \\end{matrix} \\right] \\left[ \\begin{array}{c} x_1\\\\ \\vdots\\\\ x_n\\\\ \\end{array} \\right] =\\left[ \\begin{array}{c} a_{11}x_1+\\cdots +a_{1n}x_n\\\\ \\vdots\\\\ a_{n1}x_1+\\cdots +a_{nn}x_n\\\\ \\end{array} \\right] $$ 再由“分子决定行，分母决定列，分别求导”原则（如果不理解可以返回开头再看一遍） 得到： $$ \\left[ \\begin{matrix} a_{11}\u0026amp;\t\\cdots\u0026amp;\ta_{1n}\\\\ \\vdots\u0026amp;\t\\ddots\u0026amp;\t\\vdots\\\\ a_{n1}\u0026amp;\t\\cdots\u0026amp;\ta_{nn}\\\\ \\end{matrix} \\right]=\\mathbf{A} $$ 比较简单的方法：（类似上面的证明）\n  $$ Let\\,\\, \\boldsymbol{s}=\\mathbf{A} \\mathbf{x}. \\;\\; Then, s_i=\\sum_j{\\begin{array}{c} a_{ij}x_j\\\\ \\end{array}}, and\\,\\,\\frac{\\partial s_i}{\\partial x_j}=a_{ij}. \\\\ So, \\frac{\\mathrm{d} \\boldsymbol{s}}{\\mathrm{d} \\mathbf{x}}=A. $$\n注释:有些情况（比如矩阵对向量、向量对矩阵、矩阵对矩阵求导）可能存在不好表达的情况（详情参考Reference4)因为列向量对列向量求导本质是用雅可比矩阵定义的。在这里建议直接记住结果,或者用网站验证结果；如果会用张量指标计算也行。如果你想学习更一般地做法，可以参考附录3的文章。\nReference  matrix_calculus NTU (Po-Chen Wu) Matrix Differentiation NUS(Leow Wee Kheng) 矩阵求导公式的数学推导（矩阵求导——基础篇） 机器学习中的矩阵、向量求导  ","date":"2022-01-12T21:54:58+08:00","permalink":"https://sanbuphy.github.io/p/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E7%AE%80%E6%98%93%E5%85%A5%E9%97%A8%E6%89%8B%E5%86%8C/","title":"矩阵求导简易入门手册"},{"content":"本文为个人对西瓜书不成熟的一些理解和资料整理，欢迎批评指出意见，谢谢！(可邮箱联系physicoada@gmail.com)\n西瓜书章节一 绪论 文字版浓缩可参考：周志华机器学习笔记1 by:Vay-keen\n简易版思维导图：周志华第一章 by:Sophia-11 其中一些计算问题:\np21 如何理解假设空间 考虑到通配符，假设如文中一般原始参数的取值分别为3，3，3；则总可能值为(3+1)（3+1）（3+1）+1=65种 或者更复杂一些，可以枚举求得：枚举法理解版本空间\np22 如何理解版本空间 简单而言，版本空间用于对学习内容进行收敛，是为了收敛假设空间从而使其成为与数据集一致的所有假设的子集集合。本质是缩减假设范围，也就是我们研究问题的范围。操作上可形象理解为“矩阵边界的集合”，有上下界，需要有一定的泛化程度。 从图上理解：  Photo by WIKI  从假设空间的分布缩减理解：version space算法\np24 NFL定理的推导理解  Photo by 我自己  如果还不能理解，可以参考：\n浅谈NFL没有免费的午餐定理\nNFL公式推导 如果还不能理解1/2，可参考南瓜书中的真实函数展开\n西瓜书章节二 模型评估与选择 文字浓缩版可参考：性能度量方法 假设检验\u0026amp;方差\u0026amp;偏差\n加强理解查准率、查全率以及ROC、AUC 一文带你彻底理解ROC曲线和AUC值 本文用患病的例子生动形象直观解释了所有概念。 那么，ROC、AUC具体是如何计算的呢？ 请参考南瓜书(2.20)公式，以及(2.21)。\np61 如何理解噪声与f独立从而使得最后项为0  Photo by 我自己   Photo by 我自己 \n其他问题 什么是P问题、NP问题和NPC问题\n部分内容未补全，慢慢补全\n","date":"2022-01-11T20:53:41+08:00","permalink":"https://sanbuphy.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B8%80/","title":"《机器学习》西瓜书笔记(一)"},{"content":"注意事项  在开始菜单，需要使用管理员模式打开git bash 在linux操作中（比如git）粘贴操作是shift+insert或单击鼠标的滚轮。而复制只要选中即可。（粘贴后修改就很麻烦了，推荐先修改好再粘贴）  生成SSH账号密码 如果你是第一次使用,可以先设置git的user name和email：\ngit config --global user.name \u0026quot;这里改成你的名字\u0026quot; git config --global user.email \u0026quot;这里改成你的邮箱\u0026quot; 接下来即可生成ssh密钥：（注意别把$和#注释部分也给复制进去了，只需要复制考虑$后面的部分）\n$ ssh-keygen -t rsa -b 4096 -C \u0026quot;这里改成你的邮箱\u0026quot; # -t 密钥方式设定 # -b 密钥强度设定 # -C 注释设定 # 你会看到出现以下信息： Generating public/private rsa key pair. Enter file in which to save the key (/Users/ts/.ssh/id_rsa): /Users/ts/.ssh/id_rsa_github # 此时输入你的密钥用户名(可以是邮箱) Enter passphrase (empty for no passphrase): #此时输入你的密钥密码 Enter same passphrase again: # 再次输入密码 #以防万一忘记账户密码，你可以记在其他地方 #看到以下信息，便说明你大概率生成成功 Your identification has been saved in id_rsa_github. Your public key has been saved in id_rsa_github.pub. 接下来需要检查我们是不是真的生成成功：\n$ ls -l ~/.ssh #如果你看到以下信息，就说明已生成成功（没看到config也没关系） -rw------- 1 ts staff 938 9 15 22:53 config -rw------- 1 ts staff 3326 11 8 21:52 id_rsa_github #私密密钥 -rw-r--r-- 1 ts staff 757 11 8 21:52 id_rsa_github.pub #公开密钥 注意这个要用记事本模式打开，然后在下一步骤中粘贴 注意，这时候可能找不到密钥，但在文件夹中又看得到rsa密钥文件，此时可以在不同文件夹（可能生成在某个子类文件夹内）右键打开git bash再输入上述命令，直到能出现以上信息为止。【记住此时的文件夹，在第四步还有用】\n在github添加SSH key 这一步比较简单，在github中右上角找到settings，找到SSH and GPG keys，再选择New SSH key，把上一个步骤中的公开密钥内信息全部粘贴到key中，Title可以随便写。最后点击Add key即可完成（如果想看图文操作可以参考reference）\n最后修改与验证 此时回到第二步末尾中的文件夹，输入以下代码：\n$ vim ~/.ssh/config 此时已在命令行格式中进入文件,粘贴以下讯息：\nHost github HostName github.com IdentityFile ~/.ssh/id_rsa_github #指定私密密钥 User git 粘贴后（此时还在文件中），我们需要按ESC键跳到命令模式，然后输入下列指令：\n$ :wq #冒号是必须的，意思是保存文件并退出vi 最后修正权限：\n$ chmod 600 ~/.ssh/config 接下来我们尝试连接，首先确认ssh-agent是否正常运行:\n$ eval \u0026quot;$(ssh-agent -s)\u0026quot; Agent pid 32047 # 出现类似信息则表示正常运行 $ ssh-add ~/.ssh/id_rsa_github Enter passphrase for /Users/ts/.ssh/id_rsa_github: # 此时输入第二步中设定的密码 Identity added: /Users/ts/.ssh/id_rsa_github (/Users/ts/.ssh/id_rsa_github) 最后进行连接！\n$ ssh -T git@github.com Hi mackerel7! You've successfully authenticated, but GitHub does not provide shell access. #恭喜你，当出现如上信息则表示你已经成功链接！ Reference  GitHubにssh接続できるようにする GitHub如何配置SSH Key  ","date":"2022-01-05T22:54:06+08:00","permalink":"https://sanbuphy.github.io/p/%E5%88%A9%E7%94%A8git%E7%94%9F%E6%88%90ssh%E5%B9%B6%E4%B8%8Egithub%E8%BF%9E%E6%8E%A5/","title":"利用git生成SSH并与github连接"},{"content":"机器学习的数学基础 基础不牢地动山摇，好好打数理基础！但一口吃不成大胖子—— 有答主提到：\n ”在很多相关的回答中，我都一再强调不要试图补足数学知识再开始学习机器学习。一般来说，大部分机器学习课程/书籍都要求：\n 线性代数：矩阵/张量乘法、求逆，奇异值分解/特征值分解，行列式，范数等 统计与概率：概率分布，独立性与贝叶斯，最大似然（MLE）和最大后验估计（MAP）等 优化：线性优化，非线性优化(凸优化/非凸优化)以及其衍生的求解方法如梯度下降、牛顿法、基因算法和模拟退火等 微积分：偏微分，链式法则，矩阵求导等 信息论、数值理论等\n一般人如果想要把这些知识都补全再开始机器学习往往需要很长时间，容易半途而废。而且这些知识是工具不是目的，我们的目标不是成为优化大师。建议在机器学习的过程中哪里不会补哪里，这样更有目的性且耗时更低。” [本文只包含开源部分的下载链接]    线性代数 Introduction to Linear Algebra 适合入门、相对简单友好的书\n下载地址 视频教程 台湾清华大学 趙啟超教授 课程首页\n矩阵求导相关  推荐一下我自己写的入门：矩阵求导简易入门手册 台湾大学 Matrix Calculu by Po-Chen Wu 我个人觉得是简要却齐全的速成ppt。 查阅手册：matrixcookbook 在线计算与验证：MatrixCalculus  线性代数 拓展(应用数学系) 線性代數(一) Linear Algebra I 视频地址 線性代數(二) Linear Algebra II 课程用书：Linear Algebra, 4th Edition, S. Friedberg, A. Insel and L. Spence, 2003, Prentice Hall.\n概率论与统计学 洪永淼 概率论与统计学 课件与习题解答\n Mathematics for Machine Learning 本书主页 下载地址 学习视频及其笔记 本书结构： Part I: Mathematical Foundations Introduction and Motivation\nLinear Algebra Analytic Geometry Matrix Decompositions\nVector Calculus\nProbability and Distribution Continuous Optimization\nPart II: Central Machine Learning Problems When Models Meet Data\nLinear Regression\nDimensionality Reduction with Principal Component Analysis Density Estimation with Gaussian Mixture Models\nClassification with Support Vector Machines\n机器学习入门 李宏毅2021春机器学习课程 课程地址： https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.html\n课件和资料Github版： https://github.com/Fafa-DL/Lhy_Machine_Learning 可参考笔记： https://github.com/unclestrong/DeepLearning_LHY21_Notes\n机器学习实战：基于Scikit-Learn和TensorFlow 好书，看就完了!!（翻译可能有时候不靠谱）   [涉及到的代码]](https://github.com/ageron/handson-ml2)\npython机器学习手册 本书的特色是任务制学习\n机器学习进阶 李航老师 统计学习 入门选手可参考学习路径 Photo by NLP从入门到放弃 \n深度学习 待更新（开摆）\n计算机视觉 待更新（开摆）\n Reference  如何用3个月零基础入门「机器学习」？by微 三个月从零入门深度学习，保姆级学习路线图 刘建平博客  ","date":"2021-12-31T19:48:48+08:00","image":"https://sanbuphy.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E8%B7%AF%E5%BE%84%E5%8F%8A%E5%85%B6%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/knowledge-3_hu3adb970a553883b0b7ae744c75f65cb9_2096564_120x120_fill_q75_box_smart1.jpg","permalink":"https://sanbuphy.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E8%B7%AF%E5%BE%84%E5%8F%8A%E5%85%B6%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/","title":"机器学习入门路径及其数学基础"},{"content":"前置要求  你可能需要学习如何使用git，可参考本博中的教程或观看狂神git简单教程。 你也许也想知道怎么利用GitHub Desktop上传东西到github上，可参考GitHub Desktop 的使用教程  认识hugo Hugo是由Go语言实现的静态网站生成器。简单、易用、高效、易扩展、快速部署。\n中文文档地址： https://www.gohugo.org/\n图文安装教程1\n图文安装教程2\nhugo的结构 hugo的基本用法和页面改造 hugo中文帮助文档\n皮肤下载 https://www.gohugo.org/theme/\n注：我用的是hahwul 写的stack： https://github.com/CaiJimmy/hugo-theme-stack\n主题手册\nmarkdown语法检索 https://www.appinn.com/markdown/#%E5%AE%97%E6%97%A8 常见的markdown写法\n创建你的第一个文章 使用 hugo new xxxxx.md\n注意命名时不可以空格，可以用-代替 然后就可以使用 hugo server 来查看效果啦！\n发布你的博客 我们将使用github.io来代替服务器以及域名：\n推荐参考教程 几个注意事项：\n Git要上传或执行的文件可以在文件夹中，右键空白地区点git bash here从而实现目录内操作。 在linux操作中（比如git）粘贴操作是shift+insert或单击鼠标的滚轮。而复制只要选中即可。 **【非常重要】**github的域名地址与用户名必须一致，比如你的github名字叫sakura，那么域名必须是sakura.github.io。 hugo命令hugo --baseUrl=\u0026quot;https://改为你的名字.github.io/\u0026quot;执行完后，会生成一个public文件夹，在public文件中执行1.操作即可推送。 用git推送的时候git pull --rebase origin master语句可能会出错显示没有文件，不用担心，这是因为此时目标仓库是空的，直接下一步 最后，你只需要输入对应网址，即可看到自己的宝贝博客了！  更新你的博客  在博客目录下使用hugo --baseUrl=\u0026quot;https://改为你的名字.github.io/\u0026quot;覆盖原来的public文件夹 进入public文件夹右键git bash 分别执行 git init // git add . // git commit -m \u0026lsquo;写你的备注\u0026rsquo; // git push  可能存在的问题： 界面出现404  使用Shift+F5强制刷新页面 检查域名是否和github的名字对应 github上存放文件的仓库是否只有一个分支（创建时不要勾选生成README.md) 正常public上传github仓库后会只有一个分支，且包含了public内的所有文件  文章看不到  检查是否格式正确，使用了hugo new xxxx.md 检查是否包含了draft: true，若有则删除或使用hugo server -D，若草稿模式开启是看不到文章的  数学公式不显示  是否使用了math: true，或尝试导入MathJax包，可参考Hugo に MathJax を導入して数式を書けるようにする 或者分离式的mathjax调用方法HugoでMathJaxを使う MathJax的中文文档：https://www.gohugo.org/doc/tutorials/mathjax/ Mathjax的日文文档：https://www.eng.niigata-u.ac.jp/~nomoto/download/mathjax.pdf 注意此时\\\\换行不成功的话，用\\\\\\试试看，有些\\,的无效也可以用\\\\,代替尝试。 有时候数学公式正确也会显示不出来，此时你可以检查代码界面或网页公式处是否存在斜体如\u0026quot;_j\u0026quot;，此时改为\u0026quot;_j\u0026quot;即可恢复正常，特别是_{}时要注意，可以把开始倾斜的代码（找到这里的\u0026quot;_\u0026quot;)改为_{}就可以正常显示。  文章图片加载很慢  可以参考这个文章Hugo Content 使用图源、压缩与工具介绍  文章头看到了不同的格式比如+++与\u0026mdash;  Front Matter支持三种格式，yaml，toml与json方式，你可以参考：基础文件和头部格式介绍  git push不成功  此时大概率是网络通信有问题，可以关掉git终端后科学上网；重启git 终端后（windows需要，linux系统不需要）再进行push大概率就可以解决问题了；此时无需再进行git init 等初始化操作因为之前已经做完。  ","date":"2021-12-28T18:39:12+08:00","permalink":"https://sanbuphy.github.io/p/%E5%A6%82%E4%BD%95%E8%BF%90%E7%94%A8hugo%E4%B8%8Egithub.io%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/","title":"如何运用hugo与github.io搭建个人博客"},{"content":"环境配置相关 anaconda  anaconda与Jupyter notebook安装教程https://zhuanlan.zhihu.com/p/37093476 国内的anaconda镜像下载：https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/ anaconda更新与下载包的镜像源更换：https://zhuanlan.zhihu.com/p/35985834  计算机原理 从二进制到处理器原理\nGIT小知识 要熟练使用 Git，恐怕要记住这60个命令 git 入门小知识\n其他数学 数学之美番外篇：平凡而又神奇的贝叶斯方法\n","date":"2021-12-28T18:39:12+08:00","permalink":"https://sanbuphy.github.io/p/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%E7%9F%A5%E8%AF%86%E5%BA%93/","title":"杂七杂八知识库"}]