[{"content":"初始介绍 符号规定 在本文中，我们做如下规定：\nMatrix矩阵为：$\\mathbf{A}, \\mathbf{X}, \\mathbf{Y}$ Vector向量（规定为$\\color{red} {列} $向量）为：$ \\mathbf{a}, \\mathbf{x}, \\mathbf{y}$\nScalar标量为：$a, x, y$\n分子布局 在矩阵求导中，我们有两种布局（分子与分母） 为了方便起见，本文只阐述了分子布局即：\n$$\\frac{\\partial \\mathbf{y}}{\\partial {x}}=\\left[\\begin{array}{c} \\frac{\\partial y_{1}}{\\partial x} \\\\ \\frac{\\partial y_{2}}{\\partial x} \\\\ \\vdots \\\\ \\frac{\\partial y_{m}}{\\partial x}\\end{array}\\right]\\ \\ \\ \\frac{\\partial y}{\\partial \\mathbf {x}} = \\left[\\frac{\\partial y}{\\partial x_{1}} ,\\frac{\\partial y}{\\partial x_{2}}, \\cdots ,\\frac{\\partial y}{\\partial x_{n}}\\right]$$ 分母布局为分子布局的转置。 记忆方法：分子列向量分母标量，看作长筒冰淇淋，分母看作小盒子，“能站住”。分子标量分母列向量，则盒子把冰淇淋“压倒了”。或可看最后结果的行数，是分子的行数便是分子布局。 一般的，我们会遇到如下布局,且可用记忆方法配合右图形象理解(下面是结果）：  by: Reference 2   by: Reference 2  当分子为矢量、矩阵时，结果为分子的行；当分子为标量时，结果是分母转置的行。\nVector-by-Vector 另外我们有：\n$\\mathbf{y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m \\end{bmatrix}$ $\\mathbf{x}=\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$ 由 $ \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}$ 运算后产生m行n列矩阵： $ \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\stackrel{\\text { def }}{=}\\left[\\begin{array}{cccc}\\frac{\\partial y_{1}}{\\partial x_{1}} \u0026amp; \\frac{\\partial y_{1}}{\\partial x_{2}} \u0026amp; \\ldots \u0026amp; \\frac{\\partial y_{1}}{\\partial x_{n}} \\\\ \\frac{\\partial y_{2}}{\\partial x_{1}} \u0026amp; \\frac{\\partial y_{2}}{\\partial x_{2}} \u0026amp; \\ldots \u0026amp; \\frac{\\partial y_{2}}{\\partial x_{n}} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ \\frac{\\partial y_{m}}{\\partial x_{1}} \u0026amp; \\frac{\\partial y_{m}}{\\partial x_{2}} \u0026amp; \\ldots \u0026amp; \\frac{\\partial y_{m}}{\\partial x_{n}}\\end{array}\\right] $ 这种矩阵可被称为Jacobian matrix。 接下来举个例子，若我们有： $$\\mathbf{y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\end{bmatrix} \\ \\ \\ \\mathbf{x}=\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\ \\ 且y_1=x^2_1-2x_2 \\ ,\\ y_2=x^2_3-4x_2$$ 则能得到: $$\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} =\\begin{bmatrix} 2x_1 \u0026amp; -2 \u0026amp; 0 \\\\ 0 \u0026amp; -4 \u0026amp; 2x_3 \\\\ \\end{bmatrix} $$\nMatrix-by-Scalar 同样的，我们可以给出矩阵与向量间的运算关系： $ \\frac{\\partial \\mathbf{Y}}{\\partial {x}} \\stackrel{}{=}\\left[\\begin{array}{cccc}\\frac{\\partial Y_{11}}{\\partial x} \u0026amp; \\frac{\\partial Y_{12}}{\\partial x} \u0026amp; \\ldots \u0026amp; \\frac{\\partial Y_{1n}}{\\partial x} \\\\ \\frac{\\partial Y_{21}}{\\partial x} \u0026amp; \\frac{\\partial Y_{22}}{\\partial x} \u0026amp; \\ldots \u0026amp; \\frac{\\partial Y_{2n}}{\\partial x} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ \\frac{\\partial Y_{m1}}{\\partial x} \u0026amp; \\frac{\\partial Y_{m2}}{\\partial x} \u0026amp; \\ldots \u0026amp; \\frac{\\partial Y_{mn}}{\\partial x}\\end{array}\\right]$ $ \\ \\ \\ \\ \\ and \\ \\ \\ \\ \\frac{\\partial y}{\\partial\\mathbf{X}} \\stackrel{}{=}\\left[\\begin{array}{cccc}\\frac{\\partial y}{\\partial X_{11}} \u0026amp; \\frac{\\partial y}{\\partial X_{21}} \u0026amp; \\ldots \u0026amp; \\frac{\\partial y}{\\partial X_{m1}} \\\\ \\frac{\\partial y}{\\partial X_{12}} \u0026amp; \\frac{\\partial y}{\\partial X_{22}} \u0026amp; \\ldots \u0026amp; \\frac{\\partial y}{\\partial X_{m2}} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ \\frac{\\partial y}{\\partial X_{1n}} \u0026amp; \\frac{\\partial y}{\\partial X_{2n}} \u0026amp; \\ldots \u0026amp; \\frac{\\partial y}{\\partial X_{mn}}\\end{array}\\right] $\n可以注意到当矩阵在分母时$\\mathrm{X}$已经“被转置”\n常用求导公式 注：其中$\\mathbf{a},\\mathrm{A}$都不是$\\mathbf{x}, \\mathrm{X}$的函数 $$\\frac{\\mathrm{d} \\mathbf{a}}{\\mathrm{d} x} =\\mathbf{0} \\tag{1} \\qquad (column \\ matrix)$$ $$\\frac{\\mathrm{d}a }{\\mathrm{d} \\mathbf{x}} =\\mathbf{0}^{\\mathrm{T}} \\tag{2} \\qquad (row \\ matrix)$$ $$\\frac{\\mathrm{d}a }{\\mathrm{d} \\mathbf{X}} =\\mathbf{0}^{\\mathrm{T}} \\tag{3} \\qquad (matrix)$$ $$\\frac{\\mathrm{d} \\mathbf{a} }{\\mathrm{d} \\mathbf{X}} =\\mathbf{0} \\tag{4} \\qquad (matrix)$$ $$\\frac{\\mathrm{d} \\mathbf{x} }{\\mathrm{d} \\mathbf{x}} =\\mathbf{I} \\tag{5} \\qquad (matrix)$$ 若想从“直观上”理解结果为什么会有转置符，可以反复理解 (1.2)分子布局 中的右图 $$\\frac{\\mathrm{d} \\mathbf{a}^{\\mathrm{T}}\\mathbf{x} }{\\mathrm{d} \\mathbf{x}}=\\frac{\\mathrm{d} \\mathbf{x}^{\\mathrm{T}}\\mathbf{a} }{\\mathrm{d} \\mathbf{x}}=\\mathbf{a}^{\\mathrm{T}} \\tag{6} $$ $$\\frac{\\mathrm{d} \\mathbf{x}^{\\mathrm{T}}\\mathbf{x} }{\\mathrm{d} \\mathbf{x}}=2\\mathbf{x}^{\\mathrm{T}} \\tag{7} $$ $$\\frac{\\mathrm{d} {\\mathbf{x}^{\\mathrm{T}}\\mathbf{a}}^2 }{\\mathrm{d} \\mathbf{x}}=2\\mathbf{x}^{\\mathrm{T}}\\mathbf{a}\\mathbf{a}^{\\mathrm{T}} \\tag{8} $$ $$\\frac{\\mathrm{d} \\mathbf{Ax} }{\\mathrm{d} \\mathbf{x}} =\\mathbf{A} \\tag{9} $$ $$\\frac{\\mathrm{d} \\mathbf{x}^{\\mathrm{T}}\\mathbf{A} }{\\mathrm{d} \\mathbf{x}}=\\mathbf{A}^{\\mathrm{T}} \\tag{10} $$ $$\\frac{\\mathrm{d} \\mathbf{x}^{\\mathrm{T}}\\mathbf{A}\\mathbf{x} }{\\mathrm{d} \\mathbf{x}}=\\mathbf{x}^{\\mathrm{T}}(\\mathbf{A}+\\mathbf{A}^{\\mathrm{T}}) \\tag{11} $$\n注：其中(11)用到了矩阵求导中的\u0026quot;莱布尼兹法则\u0026quot;（仔细思考前者的行列与后者的行列就可以明白）: $$\\frac{\\partial \\mathbf u^{\\mathrm{T} }\\mathbf v}{\\partial \\mathbf x} = \\mathbf u^{\\mathrm{T}} \\frac{\\partial \\mathbf v}{\\partial \\mathbf x} + \\mathbf v^{\\mathrm{T}}\\frac{\\partial \\mathbf u}{\\partial \\mathbf x}$$\n实例练习 我们会好奇一个问题：为什么大多数求导后以及求导时形式都是转置在前？ 可以这么理解，假设有参数$\\mathbf{\\Theta } =\\begin{bmatrix} \\theta_0\\\\ \\theta_1\\\\ \\theta_2 \\end{bmatrix}$ 以及列向量$\\mathbf{x}=\\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\end{bmatrix}$ 我们可以把多元函数表达为$$f(x_1,x_2)=\\Theta^{\\mathrm{T}} \\mathbf{x}=\\theta_0 + \\theta_1x_1 + \\theta_2x_2$$ 方便计算，且符合目的。在计算结果直觉中一定要紧紧把握结果与分子（上下拉长）分母（左右拉宽）的关系。\n多元线性回归推导 在西瓜书第三章开头中，我们需要求解最小error$\\ E=(\\boldsymbol{y}-\\mathbf{X}\\boldsymbol{\\hat{w}})^{\\mathrm{T}}(\\boldsymbol{y}-\\mathbf{X}\\boldsymbol{\\hat{w}}) $ 对其展开有：$$\\begin{aligned}(\\boldsymbol{y}-\\mathbf{X}\\boldsymbol{\\hat{w}})^{\\mathrm{T}}(\\boldsymbol{y}-\\mathbf{X}\\boldsymbol{\\hat{w}}) \u0026amp;=(\\boldsymbol{y}^{\\mathrm{T}}-\\boldsymbol{\\hat{w}}^{\\mathrm{T}}\\mathbf{X}^{\\mathrm{T}})(\\boldsymbol{y}-\\mathbf{X}\\boldsymbol{\\hat{w}}) \\\\ \u0026amp;= \\boldsymbol{y}^{\\mathrm{T}} \\boldsymbol{y}- (\\boldsymbol{y}^{\\mathrm{T}} \\mathbf{X}) \\boldsymbol{\\hat{w}}-\\boldsymbol{\\hat{w}}^{\\mathrm{T}} (\\mathbf{X}^{\\mathrm{T}} \\boldsymbol{y}) +\\boldsymbol{\\hat{w}}^{\\mathrm{T}} (\\mathbf{X}^{\\mathrm{T}} \\mathbf{X}) \\boldsymbol{\\hat{w}} \\end{aligned}$$ 把括号中看作常数，每一项分别对 $\\boldsymbol{\\hat{w}}$ 求导，利用公式$(9),(10),(11)$可得: $$ \\frac{\\partial \\boldsymbol{E}_{\\boldsymbol{\\hat{w}}} }{\\partial \\boldsymbol{\\hat{w}}} = -\\boldsymbol{y}^{\\mathrm{T}} \\mathbf{X} - \\boldsymbol{y}^{\\mathrm{T}} \\mathbf{X} + \\boldsymbol{\\hat{w}}^{\\mathrm{T}} [\\mathbf{X}^{\\mathrm{T}} \\mathbf{X}+(\\mathbf{X}^{\\mathrm{T}} \\mathbf{X})^{\\mathrm{T}}] = -2\\boldsymbol{y}^{\\mathrm{T}} \\mathbf{X} + 2\\boldsymbol{\\hat{w}}^{\\mathrm{T}} \\mathbf{X}^{\\mathrm{T}} \\mathbf{X} $$ 令其等于零可得： $$\\boldsymbol{y}^{\\mathrm{T}} \\mathbf{X} = \\boldsymbol{\\hat{w}}^{\\mathrm{T}} \\mathbf{X}^{\\mathrm{T}} \\mathbf{X}$$ 同时转置可得： $$ \\mathbf{X}^{\\mathrm{T}} \\boldsymbol{y} = \\mathbf{X}^{\\mathrm{T}} \\mathbf{X} \\boldsymbol{\\hat{w}} $$ 若此时 $\\mathbf{X}^{\\mathrm{T}} \\mathbf{X}$ 为满秩、非奇异矩阵，我们可以得到： $$\\boldsymbol{\\hat{w}}=( \\mathbf{X}^{\\mathrm{T}} \\mathbf{X})^{-1} \\mathbf{X}^{\\mathrm{T}} \\boldsymbol{y} \\\\ \\ \\\\ Q.E.D$$\n上文公式的证明过程 两天内补充\nReference  matrix_calculus NTU (Po-Chen Wu) Matrix Differentiation NUS(Leow Wee Kheng) 验算矩阵求导结果是否正确  ","date":"2022-01-12T21:54:58+08:00","permalink":"https://sanbuphy.github.io/p/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E7%AE%80%E6%98%93%E5%85%A5%E9%97%A8%E6%89%8B%E5%86%8C/","title":"矩阵求导简易入门手册"},{"content":"本文为个人对西瓜书不成熟的一些理解和资料整理，欢迎批评指出意见，谢谢！(可邮箱联系physicoada@gmail.com)\n西瓜书章节一 绪论 文字版浓缩可参考：周志华机器学习笔记1 by:Vay-keen\n简易版思维导图：周志华第一章 by:Sophia-11 其中一些计算问题:\np21 如何理解假设空间 考虑到通配符，假设如文中一般原始参数的取值分别为3，3，3；则总可能值为(3+1)（3+1）（3+1）+1=65种 或者更复杂一些，可以枚举求得：枚举法理解版本空间\np22 如何理解版本空间 简单而言，版本空间用于对学习内容进行收敛，是为了收敛假设空间从而使其成为与数据集一致的所有假设的子集集合。本质是缩减假设范围，也就是我们研究问题的范围。操作上可形象理解为“矩阵边界的集合”，有上下界，需要有一定的泛化程度。 从图上理解：  Photo by WIKI  从假设空间的分布缩减理解：version space算法\np24 NFL定理的推导理解  Photo by 我自己  如果还不能理解，可以参考：\n浅谈NFL没有免费的午餐定理\nNFL公式推导 如果还不能理解1/2，可参考南瓜书中的真实函数展开\n西瓜书章节二 模型评估与选择 文字浓缩版可参考：性能度量方法 假设检验\u0026amp;方差\u0026amp;偏差\n加强理解查准率、查全率以及ROC、AUC 一文带你彻底理解ROC曲线和AUC值 本文用患病的例子生动形象直观解释了所有概念。 那么，ROC、AUC具体是如何计算的呢？ 请参考南瓜书(2.20)公式，以及(2.21)。\np61 如何理解噪声与f独立从而使得最后项为0  Photo by 我自己   Photo by 我自己 \n其他问题 什么是P问题、NP问题和NPC问题\n部分内容未补全，慢慢补全\n","date":"2022-01-11T20:53:41+08:00","permalink":"https://sanbuphy.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B8%80/","title":"《机器学习》西瓜书笔记(一)"},{"content":"机器学习的数学基础 基础不牢地动山摇，好好打数理基础！但一口吃不成大胖子—— 有答主提到：\n ”在很多相关的回答中，我都一再强调不要试图补足数学知识再开始学习机器学习。一般来说，大部分机器学习课程/书籍都要求：\n 线性代数：矩阵/张量乘法、求逆，奇异值分解/特征值分解，行列式，范数等 统计与概率：概率分布，独立性与贝叶斯，最大似然（MLE）和最大后验估计（MAP）等 优化：线性优化，非线性优化(凸优化/非凸优化)以及其衍生的求解方法如梯度下降、牛顿法、基因算法和模拟退火等 微积分：偏微分，链式法则，矩阵求导等 信息论、数值理论等\n一般人如果想要把这些知识都补全再开始机器学习往往需要很长时间，容易半途而废。而且这些知识是工具不是目的，我们的目标不是成为优化大师。建议在机器学习的过程中哪里不会补哪里，这样更有目的性且耗时更低。” [本文只包含开源部分的下载链接]    线性代数 Introduction to Linear Algebra 适合入门、相对简单友好的书\n下载地址 视频教程 台湾清华大学 趙啟超教授 课程首页\n矩阵求导相关  台湾大学 Matrix Calculu by Po-Chen Wu 我个人觉得是简要却齐全的速成ppt。 查阅手册：matrixcookbook 在线计算与验证：MatrixCalculus  线性代数 拓展(应用数学系) 線性代數(一) Linear Algebra I 视频地址 線性代數(二) Linear Algebra II 课程用书：Linear Algebra, 4th Edition, S. Friedberg, A. Insel and L. Spence, 2003, Prentice Hall.\n概率论与统计学 洪永淼 概率论与统计学 课件与习题解答\n Mathematics for Machine Learning 本书主页 下载地址 学习视频及其笔记 本书结构： Part I: Mathematical Foundations Introduction and Motivation\nLinear Algebra Analytic Geometry Matrix Decompositions\nVector Calculus\nProbability and Distribution Continuous Optimization\nPart II: Central Machine Learning Problems When Models Meet Data\nLinear Regression\nDimensionality Reduction with Principal Component Analysis Density Estimation with Gaussian Mixture Models\nClassification with Support Vector Machines\n机器学习入门 李宏毅2021春机器学习课程 课程地址： https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.html\n课件和资料Github版： https://github.com/Fafa-DL/Lhy_Machine_Learning 可参考笔记： https://github.com/unclestrong/DeepLearning_LHY21_Notes\n机器学习实战：基于Scikit-Learn和TensorFlow 好书，看就完了!!（翻译可能有时候不靠谱）   [涉及到的代码]](https://github.com/ageron/handson-ml2)\npython机器学习手册 本书的特色是任务制学习\n机器学习进阶 李航老师 统计学习 入门选手可参考学习路径 Photo by NLP从入门到放弃 \n深度学习 待更新（开摆）\n计算机视觉 待更新（开摆）\n Reference  如何用3个月零基础入门「机器学习」？by微 三个月从零入门深度学习，保姆级学习路线图 刘建平博客  ","date":"2021-12-31T19:48:48+08:00","image":"https://sanbuphy.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E8%B7%AF%E5%BE%84%E5%8F%8A%E5%85%B6%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/knowledge-3_hu3adb970a553883b0b7ae744c75f65cb9_2096564_120x120_fill_q75_box_smart1.jpg","permalink":"https://sanbuphy.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E8%B7%AF%E5%BE%84%E5%8F%8A%E5%85%B6%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/","title":"机器学习入门路径及其数学基础"},{"content":"前置要求  你可能需要学习如何使用git，可参考本博中的教程或观看狂神git简单教程。 你可能需要生成ssh后与github绑定，可参考本博中的教程。 你也许也想知道怎么利用GitHub Desktop上传东西到github上，可参考GitHub Desktop 的使用教程  认识hugo Hugo是由Go语言实现的静态网站生成器。简单、易用、高效、易扩展、快速部署。\n中文文档地址： https://www.gohugo.org/\n图文安装教程1\n图文安装教程2\nhugo的结构 hugo的基本用法和页面改造 hugo中文帮助文档\n皮肤下载 https://www.gohugo.org/theme/\n注：我用的是hahwul 写的stack： https://github.com/CaiJimmy/hugo-theme-stack\n主题手册\nmarkdown语法检索 https://www.appinn.com/markdown/#%E5%AE%97%E6%97%A8\n创建你的第一个文章 使用 hugo new xxxxx.md\n注意命名时不可以空格，可以用-代替\n发布你的博客 我们将使用github.io来代替服务器以及域名：\n推荐参考教程 几个注意事项：\n Git要上传或执行的文件可以在文件夹中，右键空白地区点git bash here从而实现目录内操作。 在linux操作中（比如git）粘贴操作是shift+insert或单击鼠标的滚轮。而复制只要选中即可。 **【非常重要】**github的域名地址与用户名必须一致，比如你的github名字叫sakura，那么域名必须是sakura.github.io。 hugo命令hugo --baseUrl=\u0026quot;https://Username.github.io/\u0026quot;执行完后，会生成一个public文件夹，在public文件中执行1.操作即可推送。 用git推送的时候git pull --rebase origin master语句可能会出错显示没有文件，不用担心，这是因为此时目标仓库是空的，直接下一步 最后，你只需要输入对应网址，即可看到自己的宝贝博客了！  更新你的博客  在博客目录下使用hugo --baseUrl=\u0026quot;https://Username.github.io/\u0026quot;覆盖原来的public文件夹 进入public文件夹右键git bash 分别执行 git init // git add . // git commit -m \u0026lsquo;写你的备注\u0026rsquo; // git push  可能存在的问题： 界面出现404  使用Shift+F5强制刷新页面 检查域名是否和github的名字对应 github上存放文件的仓库是否只有一个分支（创建时不要勾选生成README.md) 正常public上传github仓库后会只有一个分支，且包含了public内的所有文件  文章看不到  检查是否格式正确，使用了hugo new xxxx.md 检查是否包含了draft: true，若有则删除或使用hugo server -D，若草稿模式开启是看不到文章的  数学公式不显示  是否使用了math: true，或尝试导入MathJax包，可参考Hugo に MathJax を導入して数式を書けるようにする 或者分离式的mathjax调用方法HugoでMathJaxを使う MathJax的中文文档：https://www.gohugo.org/doc/tutorials/mathjax/ 注意此时\\\\换行不成功的话，用\\\\\\试试看  文章图片加载很慢  可以参考这个文章Hugo Content 使用图源、压缩与工具介绍  ","date":"2021-12-28T18:39:12+08:00","permalink":"https://sanbuphy.github.io/p/%E5%A6%82%E4%BD%95%E8%BF%90%E7%94%A8hugo%E4%B8%8Egithub.io%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/","title":"如何运用hugo与github.io搭建个人博客"},{"content":"环境配置相关 anaconda  anaconda与Jupyter notebook安装教程https://zhuanlan.zhihu.com/p/37093476 国内的anaconda镜像下载：https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/ anaconda更新与下载包的镜像源更换：https://zhuanlan.zhihu.com/p/35985834  计算机原理 从二进制到处理器原理\nGIT小知识 要熟练使用 Git，恐怕要记住这60个命令 git 入门小知识\n其他数学 数学之美番外篇：平凡而又神奇的贝叶斯方法\n","date":"2021-12-28T18:39:12+08:00","permalink":"https://sanbuphy.github.io/p/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%E7%9F%A5%E8%AF%86%E5%BA%93/","title":"杂七杂八知识库"}]