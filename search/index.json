[{"content":"首先，我们需要认识什么是信息量与信息熵:\n自信息量 通常自信息可以从两个方面来理解:\n 自信息是事件发生前,事件发生的不确定性。 自信息表示事件发生后,事件所包含的信息量。  (比如你看到这，会想问作者也许不是一只猫而是人类————那我当然不是猫，这就没有不确定性可言，没有什么信息量；如果有一天我真的是猫了，那便是大新闻了！！) 自然而然，我们会想到所谓信息量应当与概率有关，且应该可以加合（两个事件发生时带来的信息量应该是分别发生时的和），也就是满足以下特质：\n $f\\left( P \\right) \u0026gt;0 \\; \\; \\; \\; P\\left( x \\right) \\in \\left( 0,1 \\right) $ $f\\left( P_A·P_B \\right) =f\\left( P_A \\right) +f\\left( P_B \\right) $ $f\\left( 1 \\right) =0$ 事件发生概率越大，自信息量越小  此时我们可以才想到对数会满足这样的性质，于是可以给出：$I\\left( x \\right) =-\\log \\left( P\\left( x \\right) \\right) $ 因为在计算机领域中习惯用二进制，所以我们通常以2为底，这样自信息量的单位就为比特bit,——即二进制数的一位包含的信息或2个选项中特别指定1个的需要信息量。而机器学习中常选择以e为底，单位为奈特nats 你可以通过以下例题来更好的理解自信息量： 以2为底的对数符号lb   \n信息熵 接下来，我们将进一步研究什么是信息熵，在前面我们学会了如何衡量一个事件的不确定性，但一个随机变量可能包含的多个事件，我们该如何对这个 随机变量的不确定性 进行刻画呢？ 我们会自然想到求出所有事件的信息量期望，且熵越大，事件的不确定性越强，当满足均匀分布时熵最大(有约束情况下要额外考虑，一阶矩二阶矩不同时的最大熵分布不同，详情可参考最大熵原理）；如果熵值小，证明某个事件发生的概率比较大，随机变量取某个值的概率大，不确定性就小了。(另外，信息熵也可以理解为解除信源不确定性所需要的信息量) 于是我们给出： (其中规定)$0\\log 0=0$ $$ H\\left( x \\right) =-\\sum_{i=1}^n{p\\left( x_i \\right) \\log \\left( p\\left( x_i \\right) \\right)} $$ 我们可以验证，当n个事件满足等概率分布时其中当结果为logn（n为总数）信息熵达到最大值。 另外可以给出条件熵(你可以运用条件概率辅助理解)： $$ H\\left( Y|X \\right) =-\\sum_x{\\sum_y{p\\left( xy \\right) \\log P\\left( y|x \\right) =\\sum_x{-\\sum_y{P\\left( y|x \\right) \\log P\\left( x \\right) =\\sum_x{P\\left( x \\right) H\\left( Y|x \\right)}}}}} $$\n相对熵 如果我们对于同一个随机变量x有两个单独的概率分布P(x) 和 Q(x)，我们可以使用KL散度（Kullback-Leibler (KL) divergence）或者叫相对熵来衡量这两个分布的差异情况，其中p对q的相对熵写作(在机器学习中，我们可以把P(x)看作真实分布，而Q(x)作为预测的分布)： $$ D_{KL}\\left(p||q \\right) =\\sum_x{p\\left( x \\right) \\log \\frac{p\\left( x \\right)}{q\\left( x \\right)}=E_{p\\left( x \\right)}\\log \\frac{p\\left( x \\right)}{q\\left( x \\right)}} $$ 同时KL散度还满足以下条件： $$ D_{KL}\\left(p||q \\right) \\ne D_{KL}(q||p) \\\\ D_{KL}\\left( p||q\\right) \\geqslant 0 $$ 对于第一个式子，我们可以借助以下内容理解：  by Deep Learning.Ian Goodfellow and Yoshua Bengio and Aaron Courville  用比较通俗的话来说，让我们回到公式之中，且注意到P(x)作为真实分布，Q(x)作为预测的分布；\n$$ D_{KL}\\left( p||q \\right) =E_{p\\left( x \\right)}\\log \\frac{p\\left( x \\right)}{q\\left( x \\right)} $$\n 当第一种情况，如果P(x)是较大的，那么q(x)也应该较大来保证相对熵最小化；如果P(x)是较小的，那实际上q(x)的大小对相对熵影响不大；所以我们只需要特别注意前者的情况。此时在看图你就可以更加理解了。  $$ D_{KL}\\left( q||p \\right) =E_{q\\left( x \\right)}\\log \\frac{q\\left( x \\right)}{p\\left( x \\right)} $$\n当第二种情况，很显然会与第一种情况相反，如果P(x)是较小的，那么q(x)也应该较小来保证相对熵最小化————这就是为什么说图中提到概率小的地方比较重要，而q(x)较大的时候就影响不大了。  交叉熵 接下来我们要了解常用的一种更常用的熵————交叉熵，由前面学到的相对熵可以进一步推导：\n$$ \\begin{aligned} D_{KL}\\left( p||q \\right) \u0026amp;=\\sum_{i=1}^n{p\\left( x_{\\mathrm{i}} \\right) \\log \\left( \\frac{p\\left( x_i \\right)}{q\\left( x_i \\right)} \\right)}\\\\ \\mathrm{ }\u0026amp;=\\sum_{i=1}^n{p\\left( x_i \\right) \\log \\left( p\\left( x_i \\right) \\right)}-\\sum_{i=1}^n{p\\left( x_i \\right)}\\log \\left( q\\left( x_i \\right) \\right)\\\\ \u0026amp;=-H\\left( p \\right) -\\sum_{i=1}^n{p\\left( x_i \\right)}\\log \\left( q\\left( x_i \\right) \\right)\\ \\end{aligned} $$\n其中第一项我们可通过推导得知是针对真实分布概率p(x)的信息熵，而后一项我们定义为交叉熵； $$ H\\left( p,q \\right) =-\\sum_{i=1}^n{p\\left( x_i \\right)}\\log \\left( q\\left( x_i \\right) \\right) $$ 交叉熵可以理解为，消除体系不确定性所需要付出的努力大小。\n交叉熵与极大似然估计的联系 由于真实分布的信息熵是确定的，在优化过程中（最小化相对熵），我们可以把他忽略，只看交叉熵的部分。此外，最小化交叉熵其实与极大似然估计是等价的，具体证明如下：（参考Deep Learning.Ian Goodfellow and Yoshua Bengio and Aaron Courville） 我们考虑一组含有m个样本的数据集$\\mathbf{X}=({ x^{(1)},\\cdots ,x^{(m)} }) $,此时可以定义 $ \\theta $ 的极大似然为(其中P为模型的联合概率)：(如果你不懂argmax是什么意思可以参考argmax科普) $$ \\begin{aligned} \\mathbf{\\theta }_{ML}\u0026amp;=\\underset{\\theta}{\\mathrm{argmax}}P_{model}\\left( \\mathbf{X};\\theta \\right)\\\\ \\mathrm{ }\u0026amp;=\\underset{\\theta}{\\mathrm{argmax}}\\prod_{i=1}^m{P_{model}\\left( \\boldsymbol{x}^{\\left( i \\right)};\\theta \\right)} \\end{aligned} $$ 由于乘积不好计算，我们可以取log将他转换为加和形式，取最值时的参数不变；且可以乘上不影响结果的 $ \\frac{1}{m} $。 $$ \\begin{aligned} \\mathbf{\\theta }_{ML}\u0026amp;=\\underset{\\theta}{\\mathrm{argmax}}\\sum_{i=1}^m \\mathrm{log} {P_{model}\\left( \\boldsymbol{x}^{\\left( i \\right)};\\theta \\right)}\\\\ \\mathrm{ }\u0026amp;=\\underset{\\theta}{\\mathrm{argmax}}\\frac{1}{m}\\sum_{i=1}^m \\mathrm{log} {P_{model}\\left( \\boldsymbol{x}^{\\left( i \\right)};\\theta \\right)} \\end{aligned} $$ 由大数定律可知（算术平均值依概率收敛于期望）： $$ \\frac{1}{m}\\sum_{i=1}^m{X_i\\longrightarrow}\\frac{1}{m}\\sum_{i=1}^m{EX_i} $$ 可以将原式进一步化为： $$ \\begin{aligned} \\mathbf{\\theta }_{ML}\u0026amp;=\\underset{\\theta}{\\mathrm{arg}\\max}\\mathbb{E}_{\\mathbf{x}~\\hat{p}_{data}}\\log P_{model}\\left( \\boldsymbol{x};\\boldsymbol{\\theta } \\right)\\\\ \u0026amp;=\\underset{\\theta}{\\mathrm{arg}\\max}\\sum_x{p\\left( x \\right) \\log q\\left( x \\right)}\\\\ \u0026amp;=\\underset{\\theta}{\\mathrm{arg}\\min}\\left[ -\\sum_x{p\\left( x \\right) \\log q\\left( x \\right)} \\right]\\\\ \\end{aligned} $$ Bravo!!! 此时你惊喜的发现这就是我们前面推导得到的交叉熵公式，至此，对于真实分布和模型分布，我们明白了MLE方法（让似然最大化）等价于两者间交叉熵的最小化。好奇的你也许想问“MLE与KL散度也是共通的吗？”————这个问题你可以自己试试看，就用上式类似办法加常数即可！\nReference  一文搞懂交叉熵损失 详解机器学习中的熵、条件熵、相对熵和交叉熵  信息论基础 数字世界逼近现实世界——浅谈分布近似与最大似然估计  ","date":"2022-01-22T22:34:11+08:00","image":"https://sanbuphy.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80/michael-sum-LEpfefQf4rU-unsplash_hu05be89c43654817bfc3a6d1d1f1925fe_2269278_120x120_fill_q75_box_smart1.jpg","permalink":"https://sanbuphy.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80/","title":"机器学习中的信息论基础"},{"content":"本文为个人对西瓜书不成熟的一些理解和资料整理，欢迎批评指出意见，谢谢！(可邮箱联系physicoada@gmail.com)\n西瓜书章节三 线性模型 文字版浓缩可参考：周志华机器学习笔记 by:Vay-keen\n简略而言，本章主要涉及到三大块内容\n 线性回归及极大似然估计 对数几率回归及交叉熵思想 二分类线性判别分析  数学基础 极大似然估计 凸函数   本图来自：凸函数的四种判断方法 多元函数判别法(Hessian矩阵正定性)：\n机器学习中凸函数的好处：函数具有唯一的极小值。这意味着我们求得的模型是全局最优，不会陷入局部最优（想象一下无数波浪的函数，难以在梯度下降中找到最小值）。\n信息熵、相对熵与交叉熵 请参考数学专栏中的文章（机器学习中的信息论基础）\n拉格朗日乘子法 线性回归 矩阵推导 矩阵求导方法及其完整过程请参考我的另外一篇文章：矩阵求导建议入门手册\n极大似然估计法 对数几率回归(逻辑回归) 极大似然估计法 线性判别分析 多分类学习 ECOC码 在书中，我们会看到这样的一张图：   其中涉及到所谓的海明距离与欧氏距离。 海明距离指的是：对于长度相等的两个字符串，在相同位置上不同字符的个数。\n而欧氏距离指的是多维空间中两点间绝对距离： $ dist\\left( X,Y \\right) =\\sqrt{\\sum_{i=1}^n{\\left( x_i-y_i \\right) ^2}} $ 由此我们可以计算出图中的数字，对(a)图中可有： $$ \\qquad \\text{示例}-1 {\\color{red} -1 +1 -1 }+1 \\\\ C1\\text{编码} \\ -1 {\\color{red} +1 -1 +1 }+1 $$ 容易看出海明距离为3，而欧氏距离为 $$ \\sqrt{\\left( -1-\\left( -1 \\right) \\right) ^2+\\left( -1-1 \\right) ^2+2^2+2^2+0}=2\\sqrt{3} $$ 其他值可以用相同的办法推导得到。\nReference  逻辑回归的原理、推导和常见问题 凸函数、损失函数、线性模型的基本形式、线性回归、w* 的代码实现   ","date":"2022-01-17T22:53:41+08:00","permalink":"https://sanbuphy.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%BA%8C/","title":"《机器学习》西瓜书笔记(二)"},{"content":"初始介绍 符号规定 在本文中，我们做如下规定：\nMatrix矩阵为：$\\mathbf{A}, \\mathbf{X}, \\mathbf{Y}$ Vector向量（规定为$\\color{red} {列} $向量）为：$ \\mathbf{a}, \\mathbf{x}, \\mathbf{y}$\nScalar标量为：$a, x, y$\n分子布局 在矩阵求导中，我们有两种布局（分子与分母） 为了方便起见，本文只阐述了分子布局即：\n$$\\frac{\\partial \\mathbf{y}}{\\partial {x}}=\\left[\\begin{array}{c} \\frac{\\partial y_{1}}{\\partial x} \\\\ \\frac{\\partial y_{2}}{\\partial x} \\\\ \\vdots \\\\ \\frac{\\partial y_{m}}{\\partial x}\\end{array}\\right]\\ \\ \\ \\frac{\\partial y}{\\partial \\mathbf {x}} = \\left[\\frac{\\partial y}{\\partial x_{1}} ,\\frac{\\partial y}{\\partial x_{2}}, \\cdots ,\\frac{\\partial y}{\\partial x_{n}}\\right]$$ 分母布局为分子布局的转置。 记忆方法：分子列向量分母标量，看作长筒冰淇淋，分母看作小盒子，“能站住”。分子标量分母列向量，则盒子把冰淇淋“压倒了”。或可看最后结果的行数，是分子的行数便是分子布局。 一般的，我们会遇到如下布局,且可用记忆方法配合右图形象理解(下面是结果）：  by: Reference 2   by: Reference 2  当分子为矢量、矩阵时，结果为分子的行；当分子为标量时，结果是分母转置的行。\nVector-by-Vector 另外我们有：\n$\\mathbf{y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m \\end{bmatrix}$ $\\mathbf{x}=\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$ 由 $ \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}$ 运算后产生m行n列矩阵： $ \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\stackrel{\\text { def }}{=}\\left[\\begin{array}{cccc}\\frac{\\partial y_{1}}{\\partial x_{1}} \u0026amp; \\frac{\\partial y_{1}}{\\partial x_{2}} \u0026amp; \\ldots \u0026amp; \\frac{\\partial y_{1}}{\\partial x_{n}} \\\\ \\frac{\\partial y_{2}}{\\partial x_{1}} \u0026amp; \\frac{\\partial y_{2}}{\\partial x_{2}} \u0026amp; \\ldots \u0026amp; \\frac{\\partial y_{2}}{\\partial x_{n}} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ \\frac{\\partial y_{m}}{\\partial x_{1}} \u0026amp; \\frac{\\partial y_{m}}{\\partial x_{2}} \u0026amp; \\ldots \u0026amp; \\frac{\\partial y_{m}}{\\partial x_{n}}\\end{array}\\right] $ 这种矩阵可被称为Jacobian matrix。 接下来举个例子，若我们有： $$\\mathbf{y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\end{bmatrix} \\ \\ \\ \\mathbf{x}=\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\ \\ 且y_1=x^2_1-2x_2 \\ ,\\ y_2=x^2_3-4x_2$$ 则能得到: $$\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} =\\begin{bmatrix} 2x_1 \u0026amp; -2 \u0026amp; 0 \\\\ 0 \u0026amp; -4 \u0026amp; 2x_3 \\\\ \\end{bmatrix} $$\nMatrix-by-Scalar 同样的，我们可以给出矩阵与向量间的运算关系： $ \\frac{\\partial \\mathbf{Y}}{\\partial {x}} \\stackrel{}{=}\\left[\\begin{array}{cccc}\\frac{\\partial Y_{11}}{\\partial x} \u0026amp; \\frac{\\partial Y_{12}}{\\partial x} \u0026amp; \\ldots \u0026amp; \\frac{\\partial Y_{1n}}{\\partial x} \\\\ \\frac{\\partial Y_{21}}{\\partial x} \u0026amp; \\frac{\\partial Y_{22}}{\\partial x} \u0026amp; \\ldots \u0026amp; \\frac{\\partial Y_{2n}}{\\partial x} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ \\frac{\\partial Y_{m1}}{\\partial x} \u0026amp; \\frac{\\partial Y_{m2}}{\\partial x} \u0026amp; \\ldots \u0026amp; \\frac{\\partial Y_{mn}}{\\partial x}\\end{array}\\right]$ $ \\ \\ \\ \\ \\ and \\ \\ \\ \\ \\frac{\\partial y}{\\partial\\mathbf{X}} \\stackrel{}{=}\\left[\\begin{array}{cccc}\\frac{\\partial y}{\\partial X_{11}} \u0026amp; \\frac{\\partial y}{\\partial X_{21}} \u0026amp; \\ldots \u0026amp; \\frac{\\partial y}{\\partial X_{m1}} \\\\ \\frac{\\partial y}{\\partial X_{12}} \u0026amp; \\frac{\\partial y}{\\partial X_{22}} \u0026amp; \\ldots \u0026amp; \\frac{\\partial y}{\\partial X_{m2}} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ \\frac{\\partial y}{\\partial X_{1n}} \u0026amp; \\frac{\\partial y}{\\partial X_{2n}} \u0026amp; \\ldots \u0026amp; \\frac{\\partial y}{\\partial X_{mn}}\\end{array}\\right] $\n可以注意到当矩阵在分母时$\\mathrm{X}$已经“被转置”\n常用求导公式 注：其中$\\mathbf{a},\\mathrm{A}$都不是$\\mathbf{x}, \\mathrm{X}$的函数 $$\\frac{\\mathrm{d} \\mathbf{a}}{\\mathrm{d} x} =\\mathbf{0} \\tag{1} \\qquad (column \\ matrix)$$ $$\\frac{\\mathrm{d}a }{\\mathrm{d} \\mathbf{x}} =\\mathbf{0}^{\\mathrm{T}} \\tag{2} \\qquad (row \\ matrix)$$ $$\\frac{\\mathrm{d}a }{\\mathrm{d} \\mathbf{X}} =\\mathbf{0}^{\\mathrm{T}} \\tag{3} \\qquad (matrix)$$ $$\\frac{\\mathrm{d} \\mathbf{a} }{\\mathrm{d} \\mathbf{X}} =\\mathbf{0} \\tag{4} \\qquad (matrix)$$ $$\\frac{\\mathrm{d} \\mathbf{x} }{\\mathrm{d} \\mathbf{x}} =\\mathbf{I} \\tag{5} \\qquad (matrix)$$ 若想从“直观上”理解结果为什么会有转置符，可以反复理解 (1.2)分子布局 中的右图 $$\\frac{\\mathrm{d} \\mathbf{a}^{\\mathrm{T}}\\mathbf{x} }{\\mathrm{d} \\mathbf{x}}=\\frac{\\mathrm{d} \\mathbf{x}^{\\mathrm{T}}\\mathbf{a} }{\\mathrm{d} \\mathbf{x}}=\\mathbf{a}^{\\mathrm{T}} \\tag{6} $$ $$\\frac{\\mathrm{d} \\mathbf{x}^{\\mathrm{T}}\\mathbf{x} }{\\mathrm{d} \\mathbf{x}}=2\\mathbf{x}^{\\mathrm{T}} \\tag{7} $$ $$\\frac{\\mathrm{d} ({\\mathbf{x}^{\\mathrm{T}}\\mathbf{a}})^2 }{\\mathrm{d} \\mathbf{x}}=2\\mathbf{x}^{\\mathrm{T}}\\mathbf{a}\\mathbf{a}^{\\mathrm{T}} \\tag{8} $$ $$\\frac{\\mathrm{d} \\mathbf{Ax} }{\\mathrm{d} \\mathbf{x}} =\\mathbf{A} \\tag{9} $$ $$\\frac{\\mathrm{d} \\mathbf{x}^{\\mathrm{T}}\\mathbf{A} }{\\mathrm{d} \\mathbf{x}}=\\mathbf{A}^{\\mathrm{T}} \\tag{10} $$ $$\\frac{\\mathrm{d} \\mathbf{x}^{\\mathrm{T}}\\mathbf{A}\\mathbf{x} }{\\mathrm{d} \\mathbf{x}}=\\mathbf{x}^{\\mathrm{T}}(\\mathbf{A}+\\mathbf{A}^{\\mathrm{T}}) \\tag{11} $$\n注：其中(11)用到了矩阵求导中的\u0026quot;莱布尼兹法则\u0026quot;（仔细思考前者的行列与后者的行列就可以明白）: $$\\frac{\\partial \\mathbf u^{\\mathrm{T} }\\mathbf v}{\\partial \\mathbf x} = \\mathbf u^{\\mathrm{T}} \\frac{\\partial \\mathbf v}{\\partial \\mathbf x} + \\mathbf v^{\\mathrm{T}}\\frac{\\partial \\mathbf u}{\\partial \\mathbf x}$$\n实例练习 我们会好奇一个问题：为什么大多数求导后以及求导时形式都是转置在前？ 可以这么理解，假设有参数$\\mathbf{\\Theta } =\\begin{bmatrix} \\theta_0\\\\ \\theta_1\\\\ \\theta_2 \\end{bmatrix}$ 以及列向量$\\mathbf{x}=\\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\end{bmatrix}$ 我们可以把多元函数表达为$$f(x_1,x_2)=\\Theta^{\\mathrm{T}} \\mathbf{x}=\\theta_0 + \\theta_1x_1 + \\theta_2x_2$$ 方便计算，且符合目的。在计算结果直觉中一定要紧紧把握结果与分子（上下拉长）分母（左右拉宽）的关系。\n多元线性回归推导 在西瓜书第三章开头中，我们需要求解最小error$\\ E=(\\boldsymbol{y}-\\mathbf{X}\\boldsymbol{\\hat{w}})^{\\mathrm{T}}(\\boldsymbol{y}-\\mathbf{X}\\boldsymbol{\\hat{w}}) $ 对其展开有：$$ \\begin{aligned}(\\boldsymbol{y}-\\mathbf{X}\\boldsymbol{\\hat{w}})^{\\mathrm{T}}(\\boldsymbol{y}-\\mathbf{X}\\boldsymbol{\\hat{w}}) \u0026amp;=(\\boldsymbol{y}^{\\mathrm{T}}-\\boldsymbol{\\hat{w}}^{\\mathrm{T}}\\mathbf{X}^{\\mathrm{T}})(\\boldsymbol{y}-\\mathbf{X}\\boldsymbol{\\hat{w}}) \\\\ \u0026amp;= \\boldsymbol{y}^{\\mathrm{T}} \\boldsymbol{y}- (\\boldsymbol{y}^{\\mathrm{T}} \\mathbf{X}) \\boldsymbol{\\hat{w}}-\\boldsymbol{\\hat{w}}^{\\mathrm{T}} (\\mathbf{X}^{\\mathrm{T}} \\boldsymbol{y}) +\\boldsymbol{\\hat{w}}^{\\mathrm{T}} (\\mathbf{X}^{\\mathrm{T}} \\mathbf{X}) \\boldsymbol{\\hat{w}} \\end{aligned}$$\n把括号中看作常数，每一项分别对 $\\boldsymbol{\\hat{w}}$ 求导，利用公式$(9),(10),(11)$可得: $$ \\frac{\\partial \\boldsymbol{E}_{\\boldsymbol{\\hat{w}}} }{\\partial \\boldsymbol{\\hat{w}}} = -\\boldsymbol{y}^{\\mathrm{T}} \\mathbf{X} - \\boldsymbol{y}^{\\mathrm{T}} \\mathbf{X} + \\boldsymbol{\\hat{w}}^{\\mathrm{T}} [\\mathbf{X}^{\\mathrm{T}} \\mathbf{X}+(\\mathbf{X}^{\\mathrm{T}} \\mathbf{X})^{\\mathrm{T}}] = -2\\boldsymbol{y}^{\\mathrm{T}} \\mathbf{X} + 2\\boldsymbol{\\hat{w}}^{\\mathrm{T}} \\mathbf{X}^{\\mathrm{T}} \\mathbf{X} $$ 令其等于零可得： $$\\boldsymbol{y}^{\\mathrm{T}} \\mathbf{X} = \\boldsymbol{\\hat{w}}^{\\mathrm{T}} \\mathbf{X}^{\\mathrm{T}} \\mathbf{X}$$ 同时转置可得： $$ \\mathbf{X}^{\\mathrm{T}} \\boldsymbol{y} = \\mathbf{X}^{\\mathrm{T}} \\mathbf{X} \\boldsymbol{\\hat{w}} $$ 若此时 $\\mathbf{X}^{\\mathrm{T}} \\mathbf{X}$ 为满秩、非奇异矩阵，我们可以得到： $$\\boldsymbol{\\hat{w}}=( \\mathbf{X}^{\\mathrm{T}} \\mathbf{X})^{-1} \\mathbf{X}^{\\mathrm{T}} \\boldsymbol{y} \\\\ \\ \\\\ Q.E.D$$\n上文公式的证明过程  如果你不知道计算结果是否正确，可以使用验算矩阵求导结果是否正确进行验证。    证明(6) $$ Let \\; s=\\boldsymbol{a}^T\\mathbf{x}={a}_1x_1+\\cdots +a_nx_n.\\quad Then,\\;\\frac{\\partial s}{\\partial x_i}=a_i \\\\ So,\\:\\frac{\\mathrm{d} \\boldsymbol{a}^{\\mathrm{T}}\\mathbf{x} }{\\mathrm{d} \\mathbf{x}}=\\frac{\\mathrm{d} s}{\\mathrm{d} \\mathbf{x}}=\\left[ \\frac{\\mathrm{d}s}{\\mathrm{d}x_1},\\frac{\\mathrm{d}s}{\\mathrm{d}x_2},\\cdots ,\\frac{\\mathrm{d}s}{\\mathrm{d}x_n} \\right] =\\left[ a_1,a_2,\\cdots ,a_n \\right] =\\boldsymbol{a}^T $$\n  证明(7) $$ Let \\; s=\\boldsymbol{\\mathbf{x}}^{\\mathrm{T}}\\mathbf{x}=\\sum_i{x_{i}^{2}}. \\quad Then,\\;\\frac{\\partial s}{\\partial x_i}=2x_i \\\\ So, \\;\\; \\frac{\\mathrm{d} s}{\\mathrm{d} \\mathbf{x}}=2\\mathbf{x}^{\\mathrm{T}} $$\n  证明(8) 当成复合函数即可，相信你可以自己证明！\n  证明(9) 比较麻烦的方法，由： $$ \\mathbf{A}\\mathbf{x}=\\left[ \\begin{matrix} a_{11}\u0026amp;\t\\cdots\u0026amp;\ta_{1n}\\\\ \\vdots\u0026amp;\t\\ddots\u0026amp;\t\\vdots\\\\ a_{n1}\u0026amp;\t\\cdots\u0026amp;\ta_{nn}\\\\ \\end{matrix} \\right] \\left[ \\begin{array}{c} x_1\\\\ \\vdots\\\\ x_n\\\\ \\end{array} \\right] =\\left[ \\begin{array}{c} a_{11}x_1+\\cdots +a_{1n}x_n\\\\ \\vdots\\\\ a_{n1}x_1+\\cdots +a_{nn}x_n\\\\ \\end{array} \\right] $$ 再由“分子决定行，分母决定列，分别求导”原则（如果不理解可以返回开头再看一遍） 得到： $$ \\left[ \\begin{matrix} a_{11}\u0026amp;\t\\cdots\u0026amp;\ta_{1n}\\\\ \\vdots\u0026amp;\t\\ddots\u0026amp;\t\\vdots\\\\ a_{n1}\u0026amp;\t\\cdots\u0026amp;\ta_{nn}\\\\ \\end{matrix} \\right]=\\mathbf{A} $$ 比较简单的方法：（类似上面的证明）\n  $$ Let\\,\\, \\boldsymbol{s}=\\mathbf{A} \\mathbf{x}. \\;\\; Then, s_i=\\sum_j{\\begin{array}{c} a_{ij}x_j\\\\ \\end{array}}, and\\,\\,\\frac{\\partial s_i}{\\partial x_j}=a_{ij}. \\\\ So, \\frac{\\mathrm{d} \\boldsymbol{s}}{\\mathrm{d} \\mathbf{x}}=A. $$\n注释:有些情况（比如矩阵对向量、向量对矩阵、矩阵对矩阵求导）可能存在不好表达的情况（详情参考Reference4)因为列向量对列向量求导本质是用雅可比矩阵定义的。在这里建议直接记住结果,或者用网站验证结果；如果会用张量指标计算也行。如果你想学习更一般地做法，可以参考附录3的文章。\nReference  matrix_calculus NTU (Po-Chen Wu) Matrix Differentiation NUS(Leow Wee Kheng) 矩阵求导公式的数学推导（矩阵求导——基础篇） 机器学习中的矩阵、向量求导  ","date":"2022-01-12T21:54:58+08:00","permalink":"https://sanbuphy.github.io/p/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E7%AE%80%E6%98%93%E5%85%A5%E9%97%A8%E6%89%8B%E5%86%8C/","title":"矩阵求导简易入门手册"},{"content":"本文为个人对西瓜书不成熟的一些理解和资料整理，欢迎批评指出意见，谢谢！(可邮箱联系physicoada@gmail.com)\n西瓜书章节一 绪论 文字版浓缩可参考：周志华机器学习笔记1 by:Vay-keen\n简易版思维导图：周志华第一章 by:Sophia-11 其中一些计算问题:\np21 如何理解假设空间 考虑到通配符，假设如文中一般原始参数的取值分别为3，3，3；则总可能值为(3+1)（3+1）（3+1）+1=65种 或者更复杂一些，可以枚举求得：枚举法理解版本空间\np22 如何理解版本空间 简单而言，版本空间用于对学习内容进行收敛，是为了收敛假设空间从而使其成为与数据集一致的所有假设的子集集合。本质是缩减假设范围，也就是我们研究问题的范围。操作上可形象理解为“矩阵边界的集合”，有上下界，需要有一定的泛化程度。 从图上理解：  Photo by WIKI  从假设空间的分布缩减理解：version space算法\np24 NFL定理的推导理解  Photo by 我自己  如果还不能理解，可以参考：\n浅谈NFL没有免费的午餐定理\nNFL公式推导 如果还不能理解1/2，可参考南瓜书中的真实函数展开\n西瓜书章节二 模型评估与选择 文字浓缩版可参考：性能度量方法 假设检验\u0026amp;方差\u0026amp;偏差\n加强理解查准率、查全率以及ROC、AUC 一文带你彻底理解ROC曲线和AUC值 本文用患病的例子生动形象直观解释了所有概念。 那么，ROC、AUC具体是如何计算的呢？ 请参考南瓜书(2.20)公式，以及(2.21)。\np61 如何理解噪声与f独立从而使得最后项为0  Photo by 我自己   Photo by 我自己 \n其他问题 什么是P问题、NP问题和NPC问题\n部分内容未补全，慢慢补全\n","date":"2022-01-11T20:53:41+08:00","permalink":"https://sanbuphy.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B8%80/","title":"《机器学习》西瓜书笔记(一)"},{"content":"注意事项  在开始菜单，需要使用管理员模式打开git bash 在linux操作中（比如git）粘贴操作是shift+insert或单击鼠标的滚轮。而复制只要选中即可。（粘贴后修改就很麻烦了，推荐先修改好再粘贴）  生成SSH账号密码 如果你是第一次使用,可以先设置git的user name和email：\ngit config --global user.name \u0026quot;这里改成你的名字\u0026quot; git config --global user.email \u0026quot;这里改成你的邮箱\u0026quot; 接下来即可生成ssh密钥：（注意别把$和#注释部分也给复制进去了，只需要复制考虑$后面的部分）\n$ ssh-keygen -t rsa -b 4096 -C \u0026quot;这里改成你的邮箱\u0026quot; # -t 密钥方式设定 # -b 密钥强度设定 # -C 注释设定 # 你会看到出现以下信息： Generating public/private rsa key pair. Enter file in which to save the key (/Users/ts/.ssh/id_rsa): /Users/ts/.ssh/id_rsa_github # 此时输入你的密钥用户名(可以是邮箱) Enter passphrase (empty for no passphrase): #此时输入你的密钥密码 Enter same passphrase again: # 再次输入密码 #以防万一忘记账户密码，你可以记在其他地方 #看到以下信息，便说明你大概率生成成功 Your identification has been saved in id_rsa_github. Your public key has been saved in id_rsa_github.pub. 接下来需要检查我们是不是真的生成成功：\n$ ls -l ~/.ssh #如果你看到以下信息，就说明已生成成功（没看到config也没关系） -rw------- 1 ts staff 938 9 15 22:53 config -rw------- 1 ts staff 3326 11 8 21:52 id_rsa_github #私密密钥 -rw-r--r-- 1 ts staff 757 11 8 21:52 id_rsa_github.pub #公开密钥 注意这个要用记事本模式打开，然后在下一步骤中粘贴 注意，这时候可能找不到密钥，但在文件夹中又看得到rsa密钥文件，此时可以在不同文件夹（可能生成在某个子类文件夹内）右键打开git bash再输入上述命令，直到能出现以上信息为止。【记住此时的文件夹，在第四步还有用】\n在github添加SSH key 这一步比较简单，在github中右上角找到settings，找到SSH and GPG keys，再选择New SSH key，把上一个步骤中的公开密钥内信息全部粘贴到key中，Title可以随便写。最后点击Add key即可完成（如果想看图文操作可以参考reference）\n最后修改与验证 此时回到第二步末尾中的文件夹，输入以下代码：\n$ vim ~/.ssh/config 此时已在命令行格式中进入文件,粘贴以下讯息：\nHost github HostName github.com IdentityFile ~/.ssh/id_rsa_github #指定私密密钥 User git 粘贴后（此时还在文件中），我们需要按ESC键跳到命令模式，然后输入下列指令：\n$ :wq #冒号是必须的，意思是保存文件并退出vi 最后修正权限：\n$ chmod 600 ~/.ssh/config 接下来我们尝试连接，首先确认ssh-agent是否正常运行:\n$ eval \u0026quot;$(ssh-agent -s)\u0026quot; Agent pid 32047 # 出现类似信息则表示正常运行 $ ssh-add ~/.ssh/id_rsa_github Enter passphrase for /Users/ts/.ssh/id_rsa_github: # 此时输入第二步中设定的密码 Identity added: /Users/ts/.ssh/id_rsa_github (/Users/ts/.ssh/id_rsa_github) 最后进行连接！\n$ ssh -T git@github.com Hi mackerel7! You've successfully authenticated, but GitHub does not provide shell access. #恭喜你，当出现如上信息则表示你已经成功链接！ Reference  GitHubにssh接続できるようにする GitHub如何配置SSH Key  ","date":"2022-01-05T22:54:06+08:00","permalink":"https://sanbuphy.github.io/p/%E5%88%A9%E7%94%A8git%E7%94%9F%E6%88%90ssh%E5%B9%B6%E4%B8%8Egithub%E8%BF%9E%E6%8E%A5/","title":"利用git生成SSH并与github连接"},{"content":"机器学习的数学基础 基础不牢地动山摇，好好打数理基础！但一口吃不成大胖子—— 有答主提到：\n ”在很多相关的回答中，我都一再强调不要试图补足数学知识再开始学习机器学习。一般来说，大部分机器学习课程/书籍都要求：\n 线性代数：矩阵/张量乘法、求逆，奇异值分解/特征值分解，行列式，范数等 统计与概率：概率分布，独立性与贝叶斯，最大似然（MLE）和最大后验估计（MAP）等 优化：线性优化，非线性优化(凸优化/非凸优化)以及其衍生的求解方法如梯度下降、牛顿法、基因算法和模拟退火等 微积分：偏微分，链式法则，矩阵求导等 信息论、数值理论等\n一般人如果想要把这些知识都补全再开始机器学习往往需要很长时间，容易半途而废。而且这些知识是工具不是目的，我们的目标不是成为优化大师。建议在机器学习的过程中哪里不会补哪里，这样更有目的性且耗时更低。” [本文只包含开源部分的下载链接]    线性代数 Introduction to Linear Algebra 适合入门、相对简单友好的书\n下载地址 视频教程 台湾清华大学 趙啟超教授 课程首页\n矩阵求导相关  推荐一下我自己写的入门：矩阵求导简易入门手册 台湾大学 Matrix Calculu by Po-Chen Wu 我个人觉得是简要却齐全的速成ppt。 查阅手册：matrixcookbook 在线计算与验证：MatrixCalculus  线性代数 拓展(应用数学系) 線性代數(一) Linear Algebra I 视频地址 線性代數(二) Linear Algebra II 课程用书：Linear Algebra, 4th Edition, S. Friedberg, A. Insel and L. Spence, 2003, Prentice Hall.\n概率论与统计学 洪永淼 概率论与统计学 课件与习题解答\n Mathematics for Machine Learning 本书主页 下载地址 学习视频及其笔记 本书结构： Part I: Mathematical Foundations Introduction and Motivation\nLinear Algebra Analytic Geometry Matrix Decompositions\nVector Calculus\nProbability and Distribution Continuous Optimization\nPart II: Central Machine Learning Problems When Models Meet Data\nLinear Regression\nDimensionality Reduction with Principal Component Analysis Density Estimation with Gaussian Mixture Models\nClassification with Support Vector Machines\n机器学习入门 李宏毅2021春机器学习课程 课程地址： https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.html\n课件和资料Github版： https://github.com/Fafa-DL/Lhy_Machine_Learning 可参考笔记： https://github.com/unclestrong/DeepLearning_LHY21_Notes\n机器学习实战：基于Scikit-Learn和TensorFlow 好书，看就完了!!（翻译可能有时候不靠谱）   [涉及到的代码]](https://github.com/ageron/handson-ml2)\npython机器学习手册 本书的特色是任务制学习\n机器学习进阶 李航老师 统计学习 入门选手可参考学习路径 Photo by NLP从入门到放弃 \n深度学习 待更新（开摆）\n计算机视觉 待更新（开摆）\n Reference  如何用3个月零基础入门「机器学习」？by微 三个月从零入门深度学习，保姆级学习路线图 刘建平博客  ","date":"2021-12-31T19:48:48+08:00","image":"https://sanbuphy.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E8%B7%AF%E5%BE%84%E5%8F%8A%E5%85%B6%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/knowledge-3_hu3adb970a553883b0b7ae744c75f65cb9_2096564_120x120_fill_q75_box_smart1.jpg","permalink":"https://sanbuphy.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E8%B7%AF%E5%BE%84%E5%8F%8A%E5%85%B6%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/","title":"机器学习入门路径及其数学基础"},{"content":"前置要求  你可能需要学习如何使用git，可参考本博中的教程或观看狂神git简单教程。 你也许也想知道怎么利用GitHub Desktop上传东西到github上，可参考GitHub Desktop 的使用教程  认识hugo Hugo是由Go语言实现的静态网站生成器。简单、易用、高效、易扩展、快速部署。\n中文文档地址： https://www.gohugo.org/\n图文安装教程1\n图文安装教程2\nhugo的结构 hugo的基本用法和页面改造 hugo中文帮助文档\n皮肤下载 https://www.gohugo.org/theme/\n注：我用的是hahwul 写的stack： https://github.com/CaiJimmy/hugo-theme-stack\n主题手册\nmarkdown语法检索 https://www.appinn.com/markdown/#%E5%AE%97%E6%97%A8 常见的markdown写法\n创建你的第一个文章 使用 hugo new xxxxx.md\n注意命名时不可以空格，可以用-代替 然后就可以使用 hugo server 来查看效果啦！\n发布你的博客 我们将使用github.io来代替服务器以及域名：\n推荐参考教程 几个注意事项：\n Git要上传或执行的文件可以在文件夹中，右键空白地区点git bash here从而实现目录内操作。 在linux操作中（比如git）粘贴操作是shift+insert或单击鼠标的滚轮。而复制只要选中即可。 **【非常重要】**github的域名地址与用户名必须一致，比如你的github名字叫sakura，那么域名必须是sakura.github.io。 hugo命令hugo --baseUrl=\u0026quot;https://改为你的名字.github.io/\u0026quot;执行完后，会生成一个public文件夹，在public文件中执行1.操作即可推送。 用git推送的时候git pull --rebase origin master语句可能会出错显示没有文件，不用担心，这是因为此时目标仓库是空的，直接下一步 最后，你只需要输入对应网址，即可看到自己的宝贝博客了！  更新你的博客  在博客目录下使用hugo --baseUrl=\u0026quot;https://改为你的名字.github.io/\u0026quot;覆盖原来的public文件夹 进入public文件夹右键git bash 分别执行 git init // git add . // git commit -m \u0026lsquo;写你的备注\u0026rsquo; // git push  可能存在的问题： 界面出现404  使用Shift+F5强制刷新页面 检查域名是否和github的名字对应 github上存放文件的仓库是否只有一个分支（创建时不要勾选生成README.md) 正常public上传github仓库后会只有一个分支，且包含了public内的所有文件  文章看不到  检查是否格式正确，使用了hugo new xxxx.md 检查是否包含了draft: true，若有则删除或使用hugo server -D，若草稿模式开启是看不到文章的  数学公式不显示  是否使用了math: true，或尝试导入MathJax包，可参考Hugo に MathJax を導入して数式を書けるようにする 或者分离式的mathjax调用方法HugoでMathJaxを使う MathJax的中文文档：https://www.gohugo.org/doc/tutorials/mathjax/ Mathjax的日文文档：https://www.eng.niigata-u.ac.jp/~nomoto/download/mathjax.pdf 注意此时\\\\换行不成功的话，用\\\\\\试试看，有些\\,的无效也可以用\\\\,代替尝试。 有时候数学公式正确也会显示不出来，此时你可以检查代码界面或网页公式处是否存在斜体如\u0026quot;_j\u0026quot;，此时改为\u0026quot;_j\u0026quot;即可恢复正常，特别是_{}时要注意，可以把开始倾斜的代码（找到这里的\u0026quot;_\u0026quot;)改为_{}就可以正常显示。  文章图片加载很慢  可以参考这个文章Hugo Content 使用图源、压缩与工具介绍  文章头看到了不同的格式比如+++与\u0026mdash;  Front Matter支持三种格式，yaml，toml与json方式，你可以参考：基础文件和头部格式介绍  ","date":"2021-12-28T18:39:12+08:00","permalink":"https://sanbuphy.github.io/p/%E5%A6%82%E4%BD%95%E8%BF%90%E7%94%A8hugo%E4%B8%8Egithub.io%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/","title":"如何运用hugo与github.io搭建个人博客"},{"content":"环境配置相关 anaconda  anaconda与Jupyter notebook安装教程https://zhuanlan.zhihu.com/p/37093476 国内的anaconda镜像下载：https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/ anaconda更新与下载包的镜像源更换：https://zhuanlan.zhihu.com/p/35985834  计算机原理 从二进制到处理器原理\nGIT小知识 要熟练使用 Git，恐怕要记住这60个命令 git 入门小知识\n其他数学 数学之美番外篇：平凡而又神奇的贝叶斯方法\n","date":"2021-12-28T18:39:12+08:00","permalink":"https://sanbuphy.github.io/p/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%E7%9F%A5%E8%AF%86%E5%BA%93/","title":"杂七杂八知识库"}]